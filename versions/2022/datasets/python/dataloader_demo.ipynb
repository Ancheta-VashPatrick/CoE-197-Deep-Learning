{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataloader Demo\n",
    "\n",
    "We illustrate how to build a custom dataset and dataloader. We will use our collected and labeled images for object detection. The `640x480` RGB images were collected using an off-the-shelf USB camera (A4TECH PK-635G).\n",
    "The images were labeled using [VIA](https://www.robots.ox.ac.uk/~vgg/software/via/) and the labels are stored in a CSV file. \n",
    "\n",
    "Please download the dataset from [here](https://bit.ly/adl2-ssd). Extract the dataset on the same directory as this file:\n",
    "\n",
    "```\n",
    "--> datasets --> python --> config.py\n",
    "                        --> dataloader_demo.ipynb\n",
    "                        --> drinks/\n",
    "                        --> label_utils.py\n",
    "```\n",
    "\n",
    "**Note**: Before running this demo, please make sure that you have `wandb.ai` free account. See our discussion on [`wandb.ai`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import** the required modules.\n",
    "\n",
    "`label_utils` is a helper module for loading the CSV file and converting the labels to string. It also contains helper functions to build the label dictionary from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "import label_utils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Login to and initialize** `wandb`. You will need to use your `wandb` API key to run this demo.\n",
    "\n",
    "We will use the following dataset and dataloader configurations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrowel\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2022-03-18 09:19:08.203784: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-18 09:19:08.203825: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rowel/github/roatienza/Deep-Learning-Experiments/versions/2022/datasets/python/wandb/run-20220318_091906-2u5jj6au</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/upeee/dataloader-project/runs/2u5jj6au\" target=\"_blank\">resilient-deluge-1</a></strong> to <a href=\"https://wandb.ai/upeee/dataloader-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "config = {\n",
    "  \"num_workers\": 2,\n",
    "  \"pin_memory\": True,\n",
    "  \"batch_size\": 8,\n",
    "  \"dataset\": \"drinks\",\n",
    "  \"train_split\": \"drinks/labels_train.csv\",\n",
    "  \"test_split\": \"drinks/labels_test.csv\",\n",
    "}\n",
    "run = wandb.init(project=\"dataloader-project\", entity=\"upeee\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader for Custom Object Detection\n",
    "\n",
    "The dataset is a list of images and their labels. The labels are stored in a CSV file using the following format.\n",
    "\n",
    "```\n",
    "frame,xmin,xmax,ymin,ymax,class_id\n",
    "0001000.jpg,310,445,104,443,1\n",
    "0000999.jpg,194,354,96,478,1\n",
    "0000998.jpg,105,383,134,244,1\n",
    "0000997.jpg,157,493,89,194,1\n",
    "0000996.jpg,51,435,207,347,1\n",
    "...\n",
    "```\n",
    "\n",
    "We will build a dictionary of `path_to_image` to `label` mapping. The `label` is a tensor of the form `xmin,xmax,ymin,ymax,class_id`. There can be multiple labels for an image since there can be multiple objects in an image.\n",
    "\n",
    "The `ImageDataset` class is a custom dataset class that loads the images and labels using the dictionary. The `ImageDataset` class is a subclass of `torch.utils.data.Dataset`.\n",
    "\n",
    "Our train and test dataloaders use the `wandb` configuration. \n",
    "\n",
    "We also create a custom `collate_fn` function to handle the labels per image. `collate_fn` pads all labels in a mini-batch to the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict, test_classes = label_utils.build_label_dictionary(config['test_split'])\n",
    "train_dict, train_classes  = label_utils.build_label_dictionary(config['train_split'])\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dictionary, transform=None):\n",
    "        self.dictionary = dictionary\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dictionary)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = list(self.dictionary.keys())[idx]\n",
    "        boxes = self.dictionary[key]\n",
    "        img = Image.open(key)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, boxes\n",
    "\n",
    "train_split = ImageDataset(train_dict, transforms.ToTensor())\n",
    "test_split = ImageDataset(test_dict, transforms.ToTensor())\n",
    "\n",
    "def collate_fn(batch):\n",
    "    maxlen = max([len(x[1]) for x in batch])    \n",
    "    images = []\n",
    "    boxes = []\n",
    "    for i in range(len(batch)):\n",
    "        img, box = batch[i]\n",
    "        images.append(img)\n",
    "        # pad with zeros if less than maxlen\n",
    "        if len(box) < maxlen:\n",
    "            box = np.concatenate((box, np.zeros((maxlen-len(box), box.shape[-1]))), axis=0)\n",
    "    \n",
    "        box = torch.from_numpy(box)\n",
    "        boxes.append(box)\n",
    "\n",
    "    return torch.stack(images, 0), torch.stack(boxes, 0)\n",
    "\n",
    "train_loader = DataLoader(train_split, \n",
    "                          batch_size=config['batch_size'], \n",
    "                          shuffle=True, \n",
    "                          num_workers=config['num_workers'], \n",
    "                          pin_memory=config['pin_memory'],\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_split,\n",
    "                         batch_size=config['batch_size'],\n",
    "                         shuffle=False,\n",
    "                         num_workers=config['num_workers'],\n",
    "                         pin_memory=config['pin_memory'],\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing sample data from train split\n",
    "\n",
    "We visualize sample images from the train split by creating a table with two columns. The first column shows the image filename while the second column shows the image and the bounding boxes. We use `wandb` to visualize the images. The annotation is stored in a list of dictionaries. One dictionary per image using `position`, `class_id`, `domain` and `box_caption` as keys. Please check the [`wandb` media documentation](https://docs.wandb.ai/guides/track/log/media) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fbad9e3c4f4198b5bc1b179575e9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.869 MB of 3.869 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.999970â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">resilient-deluge-1</strong>: <a href=\"https://wandb.ai/upeee/dataloader-project/runs/2u5jj6au\" target=\"_blank\">https://wandb.ai/upeee/dataloader-project/runs/2u5jj6au</a><br/>Synced 5 W&B file(s), 1 media file(s), 10 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220318_091906-2u5jj6au/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, boxes = next(iter(train_loader))\n",
    "class_labels = {i : label_utils.index2class(i) for i in train_classes}\n",
    "\n",
    "run.display(height=1000)\n",
    "table = wandb.Table(columns=['Image'])\n",
    "\n",
    "for image, box in zip(images, boxes):\n",
    "    dict = []\n",
    "    for i in range(box.shape[0]):\n",
    "        if box[i, -1] == 0:\n",
    "            continue\n",
    "        dict_item = {}\n",
    "        dict_item[\"position\"] = {\n",
    "            \"minX\": box[i, 0].item(),\n",
    "            \"maxX\": box[i, 1].item(),\n",
    "            \"minY\": box[i, 2].item(),\n",
    "            \"maxY\": box[i, 3].item(),\n",
    "            }   \n",
    "        dict_item[\"domain\"] =  \"pixel\"\n",
    "        dict_item[\"class_id\"] = (int)(box[i, 4].item())\n",
    "        dict_item[\"box_caption\"] = label_utils.index2class(dict_item[\"class_id\"])\n",
    "        dict.append(dict_item)\n",
    "    \n",
    "    img = wandb.Image(image, boxes={\n",
    "        \"ground_truth\": {\n",
    "            \"box_data\": dict,\n",
    "            \"class_labels\": class_labels\n",
    "            }\n",
    "        })\n",
    "    table.add_data(img)\n",
    "\n",
    "wandb.log({\"train_loader\": table})\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52772734322c44a04e342c358be70f2ff4d97da358cb3cd38ceb0f6066598be5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
