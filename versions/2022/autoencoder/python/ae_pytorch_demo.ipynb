{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder PyTorch Demo using MNIST\n",
    "\n",
    "In this demo, we build a simple autoencoder using PyTorch. A separate encoder and decoder are built. The encoder is trained to encode the input data into a latent space. The decoder is trained to reconstruct the input data from the latent space.\n",
    "\n",
    "This demo also shows how to use an autoencoder to remove noise from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 12:56:40.888059: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "from argparse import ArgumentParser\n",
    "from pytorch_lightning import LightningModule, Trainer, Callback\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Encoder using PyTorch\n",
    "\n",
    "We use 3 CNN layers to encode the input image. We use stride of 2 to reduce the feature map size. The last MLP layer resizes the flattened feature map to the target latent vector size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.shape: torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_features=1, kernel_size=3, n_filters=32, feature_dim=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_features, n_filters, kernel_size=kernel_size, stride=2)\n",
    "        self.conv2 = nn.Conv2d(n_filters, n_filters*2, kernel_size=kernel_size, stride=2)\n",
    "        self.conv3 = nn.Conv2d(n_filters*2, n_filters*4, kernel_size=kernel_size, stride=2)\n",
    "        self.fc1 = nn.Linear(512, feature_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = nn.ReLU()(self.conv1(x))\n",
    "        y = nn.ReLU()(self.conv2(y))\n",
    "        y = nn.ReLU()(self.conv3(y))\n",
    "        y = rearrange(y, 'b c h w -> b (c h w)')\n",
    "\n",
    "        y = self.fc1(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "# use this to get the correct input shape for  fc1. \n",
    "encoder = Encoder(n_features=1)\n",
    "x = torch.Tensor(1, 1, 28, 28)\n",
    "h = encoder(x)\n",
    "print(\"h.shape:\", h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Decoder using PyTorch\n",
    "\n",
    "A decoder is used to reconstruct the input image. The decoder is trained to reconstruct the input data from the latent space. The architecture is similar to the encoder but inverted. A latent vector is resized using an MLP layer so that it is suitable for a convolutional layer. We use strided tranposed convolutional layers to upsample the feature map until the desired image size is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tilde.shape: torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, kernel_size=3, n_filters=64, feature_dim=16, output_size=28, output_channels=1):\n",
    "        super().__init__()\n",
    "        self.init_size = output_size // 2**2 - 1\n",
    "        self.fc1 = nn.Linear(feature_dim, self.init_size**2 * n_filters)\n",
    "        # output size of conv2dtranspose is (h-1)*2 + 1 + (kernel_size - 1)\n",
    "        self.conv1 = nn.ConvTranspose2d(n_filters, n_filters//2, kernel_size=kernel_size, stride=2)\n",
    "        self.conv2 = nn.ConvTranspose2d(n_filters//2, n_filters//4, kernel_size=kernel_size, stride=2)\n",
    "        self.conv3 = nn.ConvTranspose2d(n_filters//4, output_channels, kernel_size=kernel_size-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, _ = x.shape\n",
    "        y = self.fc1(x)\n",
    "        y = rearrange(y, 'b (c h w) -> b c h w', b=B, h=self.init_size, w=self.init_size)\n",
    "        y = nn.ReLU()(self.conv1(y))\n",
    "        y = nn.ReLU()(self.conv2(y))\n",
    "        y = nn.Sigmoid()(self.conv3(y))\n",
    "\n",
    "        return y\n",
    "\n",
    "decoder = Decoder()\n",
    "x_tilde = decoder(h)\n",
    "print(\"x_tilde.shape:\", x_tilde.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Lightning AutoEncoder\n",
    "\n",
    "An autoencoder is simply an encoder and a decoder. The encoder extracts the feature vector from the input image. The decoder reconstructs the input image from the feature vector. The feature vector is the compressed representation of the input image.\n",
    "\n",
    "Our PL module can also perform denoising. Below, we also present the collate function for clean and noisy images. To generate noisy images, we apply a Gaussian noise with mean of 0.5 and a standard deviation of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_collate_fn(batch):\n",
    "        x, _ = zip(*batch)\n",
    "        x = torch.stack(x, dim=0)\n",
    "        # mean=0.5, std=0.5 normal noise\n",
    "        noise = torch.normal(0.5, 0.5, size=x.shape)\n",
    "        xn = x + noise\n",
    "        xn = torch.clamp(xn, 0, 1)\n",
    "        return xn, x\n",
    "\n",
    "def clean_collate_fn(batch):\n",
    "        x, _ = zip(*batch)\n",
    "        x = torch.stack(x, dim=0)\n",
    "        return x, x\n",
    "\n",
    "class LitAEMNISTModel(LightningModule):\n",
    "    def __init__(self, feature_dim=16, lr=0.001, batch_size=64,\n",
    "                 num_workers=4, max_epochs=30, denoise=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Encoder(feature_dim=feature_dim)\n",
    "        self.decoder = Decoder(feature_dim=feature_dim)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.denoise = denoise\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        x_tilde = self.decoder(h)\n",
    "        return x_tilde\n",
    "\n",
    "    # this is called during fit()\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_in, x = batch\n",
    "        x_tilde = self.forward(x_in)\n",
    "        loss = self.loss(x_tilde, x)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    # calls to self.log() are recorded in wandb\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"train_loss\", avg_loss, on_epoch=True)\n",
    "\n",
    "    # this is called at the end of an epoch\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x_in, x = batch\n",
    "        x_tilde = self.forward(x_in)\n",
    "        loss = self.loss(x_tilde, x)\n",
    "        return {\"x_in\" : x_in, \"x_tilde\" : x_tilde, \"test_loss\" : loss,}\n",
    "\n",
    "    # this is called at the end of all epochs\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    # validation is the same as test\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "       return self.test_step(batch, batch_idx)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.test_epoch_end(outputs)\n",
    "\n",
    "    # we use Adam optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        # this decays the learning rate to 0 after max_epochs using cosine annealing\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    # this is called after model instatiation to initiliaze the datasets and dataloaders\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataloader()\n",
    "        self.test_dataloader()\n",
    "\n",
    "    # build train and test dataloaders using MNIST dataset\n",
    "    # we use simple ToTensor transform\n",
    "    def train_dataloader(self):        \n",
    "        collate_fn = noise_collate_fn if self.denoise else clean_collate_fn\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                \"./data\", train=True, download=True, \n",
    "                transform=torchvision.transforms.ToTensor()\n",
    "            ),\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        collate_fn = noise_collate_fn if self.denoise else clean_collate_fn\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                \"./data\", train=False, download=True, \n",
    "                transform=torchvision.transforms.ToTensor()\n",
    "            ),\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = ArgumentParser(description=\"PyTorch Lightning AE MNIST Example\")\n",
    "    parser.add_argument(\"--max-epochs\", type=int, default=30, help=\"num epochs\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=64, help=\"batch size\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate\")\n",
    "\n",
    "    parser.add_argument(\"--feature-dim\", type=int, default=16, help=\"ae feature dimension\")\n",
    "    # if denoise is true\n",
    "    parser.add_argument(\"--denoise\", action=\"store_true\", help=\"denoise\")\n",
    "\n",
    "    parser.add_argument(\"--devices\", default=1)\n",
    "    parser.add_argument(\"--accelerator\", default='gpu')\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=4, help=\"num workers\")\n",
    "    \n",
    "    args = parser.parse_args(\"\")\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights and Biases Callback\n",
    "\n",
    "The callback logs train and validation metrics to `wandb`. It also logs sample predictions. This is similar to our `WandbCallback` example for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbCallback(Callback):\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        # process first 10 images of the first batch\n",
    "        if batch_idx == 0:\n",
    "            x, _ = batch\n",
    "            n = 10\n",
    "            outputs = outputs[\"x_tilde\"]\n",
    "            columns = ['image']\n",
    "            if pl_module.denoise:\n",
    "                columns += ['denoised']\n",
    "                key = \"mnist-ae-denoising\"\n",
    "            else:\n",
    "                columns += [\"reconstruction\"]\n",
    "                key = \"mnist-ae-reconstruction\"\n",
    "            data = [[wandb.Image(x_i), wandb.Image(x_tilde_i)] for x_i, x_tilde_i in list(zip(x[:n], outputs[:n]))]\n",
    "            wandb_logger.log_table(key=key, columns=columns, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training an AE\n",
    "\n",
    "We train the autoencoder on the MNIST dataset. For simple reconstruction, the input image is also the target image. For denoising, the input is the noisy image while the target is the clean image.\n",
    "\n",
    "The results can be viewed on [wandb](https://app.wandb.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    ae = LitAEMNISTModel(feature_dim=args.feature_dim, lr=args.lr, \n",
    "                         batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "                         denoise=args.denoise, max_epochs=args.max_epochs)\n",
    "    ae.setup()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrowel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2022-05-25 12:56:46.453927: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rowel/github/roatienza/Deep-Learning-Experiments/versions/2022/autoencoder/python/wandb/run-20220525_125645-gfv79752</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rowel/ae-mnist/runs/gfv79752\" target=\"_blank\">driven-eon-8</a></strong> to <a href=\"https://wandb.ai/rowel/ae-mnist\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 100 K \n",
      "1 | decoder | Decoder | 62.3 K\n",
      "2 | loss    | MSELoss | 0     \n",
      "------------------------------------\n",
      "163 K     Trainable params\n",
      "0         Non-trainable params\n",
      "163 K     Total params\n",
      "0.653     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8e3e90e0b0455997ba8ce658dbc45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb29527c44b4ce9abf0f7a3e43bdd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f72b37fdb314340bfad197507453bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f74d3365eb44dab415fb868ffdf273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe7a149eb6c4f008352695e4e5aa46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105e8f27216d4b858c5d9e4515a0dd24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d9bb7898af4a0a9eff7953504e75d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10db5dec1de24786a71b7a27fcc69ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"ae-mnist\")\n",
    "    start_time = time.time()\n",
    "    trainer = Trainer(accelerator=args.accelerator,\n",
    "                      devices=args.devices,\n",
    "                      max_epochs=args.max_epochs,\n",
    "                      #logger=wandb_logger,\n",
    "                      callbacks=[WandbCallback()])\n",
    "    trainer.fit(ae)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed time: {}\".format(elapsed_time))\n",
    "\n",
    "    wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52772734322c44a04e342c358be70f2ff4d97da358cb3cd38ceb0f6066598be5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
