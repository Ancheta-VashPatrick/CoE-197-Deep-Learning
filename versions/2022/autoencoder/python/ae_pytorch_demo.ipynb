{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder PyTorch Demo using MNIST\n",
    "\n",
    "In this demo, we build a simple autoencoder using PyTorch. A separate encoder and decoder are built. The encoder is trained to encode the input data into a latent space. The decoder is trained to reconstruct the input data from the latent space.\n",
    "\n",
    "This demo also shows how to use an autoencoder to remove noise from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 08:49:41.025216: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "from argparse import ArgumentParser\n",
    "from pytorch_lightning import LightningModule, Trainer, Callback\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Encoder using PyTorch\n",
    "\n",
    "We use 3 CNN layers to encode the input image. We use stride of 2 to reduce the feature map size. The last MLP layer resizes the flattened feature map to the target latent vector size, `feature_dim`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.shape: torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_features=1, kernel_size=3, n_filters=16, feature_dim=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_features, n_filters, kernel_size=kernel_size, stride=2)\n",
    "        self.conv2 = nn.Conv2d(n_filters, n_filters*2, kernel_size=kernel_size, stride=2)\n",
    "        self.conv3 = nn.Conv2d(n_filters*2, n_filters*4, kernel_size=kernel_size, stride=2)\n",
    "        self.fc1 = nn.Linear(256, feature_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = nn.ReLU()(self.conv1(x))\n",
    "        y = nn.ReLU()(self.conv2(y))\n",
    "        y = nn.ReLU()(self.conv3(y))\n",
    "        y = rearrange(y, 'b c h w -> b (c h w)')\n",
    "\n",
    "        y = self.fc1(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "# use this to get the correct input shape for  fc1. \n",
    "encoder = Encoder(n_features=1)\n",
    "x = torch.Tensor(1, 1, 28, 28)\n",
    "h = encoder(x)\n",
    "print(\"h.shape:\", h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Decoder using PyTorch\n",
    "\n",
    "A decoder is used to reconstruct the input image. The decoder is trained to reconstruct the input data from the latent space. The architecture is similar to the encoder but inverted. A latent vector is resized using an MLP layer so that it is suitable for a convolutional layer. We use strided tranposed convolutional layers to upsample the feature map until the desired image size is reached. The last activation layer is a sigmoid to ensure the output is in the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tilde.shape: torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, kernel_size=3, n_filters=64, feature_dim=16, output_size=28, output_channels=1):\n",
    "        super().__init__()\n",
    "        self.init_size = output_size // 2**2 - 1\n",
    "        self.fc1 = nn.Linear(feature_dim, self.init_size**2 * n_filters)\n",
    "        # output size of conv2dtranspose is (h-1)*2 + 1 + (kernel_size - 1)\n",
    "        self.conv1 = nn.ConvTranspose2d(n_filters, n_filters//2, kernel_size=kernel_size, stride=2)\n",
    "        self.conv2 = nn.ConvTranspose2d(n_filters//2, n_filters//4, kernel_size=kernel_size, stride=2)\n",
    "        self.conv3 = nn.ConvTranspose2d(n_filters//4, output_channels, kernel_size=kernel_size-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, _ = x.shape\n",
    "        y = self.fc1(x)\n",
    "        y = rearrange(y, 'b (c h w) -> b c h w', b=B, h=self.init_size, w=self.init_size)\n",
    "        y = nn.ReLU()(self.conv1(y))\n",
    "        y = nn.ReLU()(self.conv2(y))\n",
    "        y = nn.Sigmoid()(self.conv3(y))\n",
    "\n",
    "        return y\n",
    "\n",
    "decoder = Decoder()\n",
    "x_tilde = decoder(h)\n",
    "print(\"x_tilde.shape:\", x_tilde.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Lightning AutoEncoder\n",
    "\n",
    "An autoencoder is simply an encoder and a decoder. The encoder extracts the feature vector from the input image. The decoder reconstructs the input image from the feature vector. The feature vector is the compressed representation of the input image.\n",
    "\n",
    "Our PL module can also perform denoising. Below, we also present the collate function for clean and noisy images. To generate noisy images, we apply a Gaussian noise with mean of 0.5 and a standard deviation of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_collate_fn(batch):\n",
    "        x, _ = zip(*batch)\n",
    "        x = torch.stack(x, dim=0)\n",
    "        # mean=0.5, std=0.5 normal noise\n",
    "        noise = torch.normal(0.5, 0.5, size=x.shape)\n",
    "        xn = x + noise\n",
    "        xn = torch.clamp(xn, 0, 1)\n",
    "        return xn, x\n",
    "\n",
    "def clean_collate_fn(batch):\n",
    "        x, _ = zip(*batch)\n",
    "        x = torch.stack(x, dim=0)\n",
    "        return x, x\n",
    "\n",
    "class LitAEMNISTModel(LightningModule):\n",
    "    def __init__(self, feature_dim=16, lr=0.001, batch_size=64,\n",
    "                 num_workers=4, max_epochs=30, denoise=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = Encoder(feature_dim=feature_dim)\n",
    "        self.decoder = Decoder(feature_dim=feature_dim)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.denoise = denoise\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        x_tilde = self.decoder(h)\n",
    "        return x_tilde\n",
    "\n",
    "    # this is called during fit()\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_in, x = batch\n",
    "        x_tilde = self.forward(x_in)\n",
    "        loss = self.loss(x_tilde, x)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    # calls to self.log() are recorded in wandb\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        self.log(\"train_loss\", avg_loss, on_epoch=True)\n",
    "\n",
    "    # this is called at the end of an epoch\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x_in, x = batch\n",
    "        x_tilde = self.forward(x_in)\n",
    "        loss = self.loss(x_tilde, x)\n",
    "        return {\"x_in\" : x_in, \"x_tilde\" : x_tilde, \"test_loss\" : loss,}\n",
    "\n",
    "    # this is called at the end of all epochs\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        self.log(\"test_loss\", avg_loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    # validation is the same as test\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "       return self.test_step(batch, batch_idx)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.test_epoch_end(outputs)\n",
    "\n",
    "    # we use Adam optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        # this decays the learning rate to 0 after max_epochs using cosine annealing\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    # this is called after model instatiation to initiliaze the datasets and dataloaders\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataloader()\n",
    "        self.test_dataloader()\n",
    "\n",
    "    # build train and test dataloaders using MNIST dataset\n",
    "    # we use simple ToTensor transform\n",
    "    def train_dataloader(self):        \n",
    "        collate_fn = noise_collate_fn if self.denoise else clean_collate_fn\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                \"./data\", train=True, download=True, \n",
    "                transform=torchvision.transforms.ToTensor()\n",
    "            ),\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        collate_fn = noise_collate_fn if self.denoise else clean_collate_fn\n",
    "        return torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                \"./data\", train=False, download=True, \n",
    "                transform=torchvision.transforms.ToTensor()\n",
    "            ),\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments\n",
    "\n",
    "The arguments are as in our previous examples. The only new argument is `feature_dim`. This is the size of the latent vector. To use the denoising autoencoder, we also need to set `denoise` to `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = ArgumentParser(description=\"PyTorch Lightning AE MNIST Example\")\n",
    "    parser.add_argument(\"--max-epochs\", type=int, default=30, help=\"num epochs\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=64, help=\"batch size\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate\")\n",
    "\n",
    "    parser.add_argument(\"--feature-dim\", type=int, default=2, help=\"ae feature dimension\")\n",
    "    # if denoise is true\n",
    "    parser.add_argument(\"--denoise\", action=\"store_true\", help=\"train a denoising AE\")\n",
    "\n",
    "    parser.add_argument(\"--devices\", default=1)\n",
    "    parser.add_argument(\"--accelerator\", default='gpu')\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=4, help=\"num workers\")\n",
    "    \n",
    "    args = parser.parse_args(\"\")\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights and Biases Callback\n",
    "\n",
    "The callback logs train and validation metrics to `wandb`. It also logs sample predictions. This is similar to our `WandbCallback` example for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbCallback(Callback):\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        # process first 10 images of the first batch\n",
    "        if batch_idx == 0:\n",
    "            x, _ = batch\n",
    "            n = 10\n",
    "            outputs = outputs[\"x_tilde\"]\n",
    "            columns = ['image']\n",
    "            if pl_module.denoise:\n",
    "                columns += ['denoised']\n",
    "                key = \"mnist-ae-denoising\"\n",
    "            else:\n",
    "                columns += [\"reconstruction\"]\n",
    "                key = \"mnist-ae-reconstruction\"\n",
    "            data = [[wandb.Image(x_i), wandb.Image(x_tilde_i)] for x_i, x_tilde_i in list(zip(x[:n], outputs[:n]))]\n",
    "            wandb_logger.log_table(key=key, columns=columns, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training an AE\n",
    "\n",
    "We train the autoencoder on the MNIST dataset. For simple reconstruction, the input image is also the target image. For denoising, the input is the noisy image while the target is the clean image.\n",
    "\n",
    "The results can be viewed on [wandb](https://app.wandb.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    ae = LitAEMNISTModel(feature_dim=args.feature_dim, lr=args.lr, \n",
    "                         batch_size=args.batch_size, num_workers=args.num_workers,\n",
    "                         denoise=args.denoise, max_epochs=args.max_epochs)\n",
    "    ae.setup()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrowel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2022-05-29 08:49:47.276919: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.17 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rowel/github/roatienza/Deep-Learning-Experiments/versions/2022/autoencoder/python/wandb/run-20220529_084946-3qlaljaz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rowel/ae-mnist/runs/3qlaljaz\" target=\"_blank\">distinctive-waterfall-24</a></strong> to <a href=\"https://wandb.ai/rowel/ae-mnist\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 23.8 K\n",
      "1 | decoder | Decoder | 30.1 K\n",
      "2 | loss    | MSELoss | 0     \n",
      "------------------------------------\n",
      "53.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "53.9 K    Total params\n",
      "0.215     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0477faf83a504823abda33ab9f610f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b7e7973b1540aebe6f01d383d8031a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650a2d0e22274262aababc7d48be56a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339a1bc0a7594d0b9f9e445cc075d3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f326f627c148e6ab6052cd3f332611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e1aced0ba346d186601ccd3ced7a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e116be59d44477822263a731d3993c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f422d3fd904f659e2531379dfeb9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8a4ac32ffc4e63b0bc376a9008d429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95beb10163684b05a7b8c30cd97c64cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8706bdaa0eb848e39589e9f31981d8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde62e8459af4f3b8f33d0581629e167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befb252ef62646ca97a667c498a72b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4628054caa0447c9bd9409e8ccf4eca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbddc2f6cb2344b2967ef457cc5d8ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2502538c9ea482caf3dee590565bf50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87c4c4a66a44baf9556fcb9ea584fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c9fbaaab974943b6682728c804a47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31265d1676924c8293919770178ded57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f7481c57114685ad2dc80c60594a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f922036afe4bc4ae416e0f7c5ebf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe98c91ac0d40ad815db3bf6d84c9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f62df033f50471eb49344b0b92fa5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3266e28643f14cba9e2fa8310e059f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f554011a9cc14a39bbbaa858528cb0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1daa87b08445d0b852867244783c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c9081a14884cf8aec29280f490dc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d1d82bd42744a3965dee56ca1f6983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a1ece3343d4f02be2fd8a62b6616f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660427f36288410c8c15887a5b657164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bd46cfff6d46cd834b113aae191ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39236a329eab4a58ab8cb22c5f1893bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 363.13006234169006\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ac60331f4f4b2187901608d183402a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.481 MB of 0.481 MB uploaded (0.004 MB deduped)\\r'), FloatProgress(value=0.999504…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>test_loss</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>test_loss</td><td>0.03919</td></tr><tr><td>train_loss</td><td>0.0385</td></tr><tr><td>trainer/global_step</td><td>28139</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">distinctive-waterfall-24</strong>: <a href=\"https://wandb.ai/rowel/ae-mnist/runs/3qlaljaz\" target=\"_blank\">https://wandb.ai/rowel/ae-mnist/runs/3qlaljaz</a><br/>Synced 7 W&B file(s), 31 media file(s), 351 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220529_084946-3qlaljaz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"ae-mnist\")\n",
    "    start_time = time.time()\n",
    "    trainer = Trainer(accelerator=args.accelerator,\n",
    "                      devices=args.devices,\n",
    "                      max_epochs=args.max_epochs,\n",
    "                      logger=wandb_logger,\n",
    "                      callbacks=[WandbCallback()])\n",
    "    trainer.fit(ae)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed time: {}\".format(elapsed_time))\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAKc0lEQVR4nO3dyU9UWxvF4V0gPYiCREARDCBolKhE48CRYeDcRP9Dx+pEB8aRA6KS2MReEZTOBgEBaSzhju438qz3S9WtsAp/z3S54ZSwchLe7L1z29vbCYCfip1+AAB/RjkBU5QTMEU5AVOUEzC1R4W5XI4/5f5BLpcr2fo9e+SPJP3+/VvmW1tbBT3Tv0r51/uKCv0uKPbZXUW/L1tbW3/8B7w5AVOUEzBFOQFTlBMwRTkBU5QTMEU5AVM5NddizlkalZWVmVmxc8bdOivczba3t5lzAuWEcgKmKCdginICpignYIpyAqYoJ2BKbx78S1VVVcm8paVF5r29vTJXezJ//fol137//l3m8/PzMt/c3JS5+v7RDDbK/9b9nIXizQmYopyAKcoJmKKcgCnKCZiinICpv3KUEh1V2NjYKPORkRGZV1dXy1yNHJaWluTa6Nl+/Pgh8+jZ1tfXM7NozJPP52WutsqlVNpRSvQzL+WRoIUepcqbEzBFOQFTlBMwRTkBU5QTMEU5AVOUEzBVtkdjRtuP1FV6R44ckWsbGhpkfvXqVZnv3btX5gsLC5nZgwcP5NpiPX36VObq2aPtatGMNZpzrq6uZmbR1YelnFMWiysAgV2GcgKmKCdginICpignYIpyAqYoJ2CqbPdzFnNMY7Qn8ty5czKP5nXNzc0yV3sq+/r65NqNjQ2Zv337VubDw8MyHx0dzczU7DillOrr62Ue7ddU/y/R547moDup0Bksb07AFOUETFFOwBTlBExRTsAU5QRMUU7AVNnOOaM9cmrm1tHRIddGs8ahoSGZR3tNP3/+nJlF87poT+Xc3JzMo1ml2s85Ozsr10ZXIy4uLspczTmjM3OjGarzfs8svDkBU5QTMEU5AVOUEzBFOQFTlBMwZTtKibZlRSMBNS45ceKEXHvq1CmZ9/T0yFxdo5dSSsvLy5nZysqKXPv+/XuZr62tyfzjx48yr6mpycyiIz9Lua3LeUtYqfDmBExRTsAU5QRMUU7AFOUETFFOwBTlBEzJYWG0LauU23Ci7x3NQdWc89ChQ3JtlKtZYEop3b9/X+aPHj3KzKIr+iYmJmQebVcr5urEurq6gtemVNwVgNHXjn4X8/l8UeuLEf0uZ+HNCZiinIApygmYopyAKcoJmKKcgCnKCZiSw6OdPE4wOuowmtepa/gGBgbk2u7ubplPTk7K/O7duzJ/+PBhZhYdfbm5uSnzaJ4XzRrVnk01h/x/vnZtba3M+/v7M7OZmRm5dnp6WubR73K0X7SYLnAFILDLUE7AFOUETFFOwBTlBExRTsAU5QRM2Z5bG82Goj2Vly5dyswuXrwo10azRHWFX0opTU1NyTw611bZv3+/zKP5bzSLbGxszMx6e3vl2mjPZVVVlcyHh4czs2gfa3S94JMnT2T++vVrmUdXEJYCb07AFOUETFFOwBTlBExRTsAU5QRMUU7AlO2cMzrrs6mpSeZqX2I0j4tmZi9fvpT5/Py8zNW+yGgfa0NDg8xbWlpkfvjwYZmfPXs2M1P7LVNKqbq6WubPnz+Xudpne+zYMbn21atXMh8bG5N5tNeUOSeA/6GcgCnKCZiinIApygmYopyAKdsrAKNxx8mTJ2WuRgrRqOPFixcy//btm8yjcYga87S1tRW8NqWU2tvbZR5ttVNXJ0bHR6q1KcW/T2fOnMnMfv78KddGI6ToWM+bN2/K/NmzZ5lZ9PMuFG9OwBTlBExRTsAU5QRMUU7AFOUETFFOwJTtFYDR946OgFRXwp0+fVqujeZxKysrMt/Y2JB5tN1NGR8fl3l0dObIyIjM1Wf7+vWrXBv9TKKfqZo/qyM7U0ppcHBQ5tHVitFsW213Y84J/GUoJ2CKcgKmKCdginICpignYIpyAqZsj8aMrqp78+aNzC9cuJCZRXsah4aGZB7NGqM5qbpiMFp7/vx5mUf7GqOr7tT1hp2dnXLtrVu3ZB4dy3n06NHMLLq2UV0fmFI8/40+2759+zKzaH9voXhzAqYoJ2CKcgKmKCdginICpignYIpyAqZs55zRGanRHLSYc2ujM1KjfYutra0yX1paysyizx3tiezq6pJ5NKOdmZnJzL58+SLXRtfkTU1NyVzNeKM5ZrSnMvp9iZ49n89nZtFsulC8OQFTlBMwRTkBU5QTMEU5AVOUEzBFOQFTtnPOaG4VzaXUPZZqZpVSPBOL9lRGeyY/ffqUmUVnw6o9jynFzx7NYNX5rc3NzXLt8vKyzKP5sdrvGc1Yo2eL7nuNZtdKNOcs9Gvz5gRMUU7AFOUETFFOwBTlBExRTsCU7Sgl+vNz9KdxNc7o7e2Va7u7u2V+8OBBmS8sLMh8bm4uM7t+/bpcG40romeP/uyv8ugavehrt7e3y1w9e0dHh1wbHU85Ozsrc3UkaEr6s0Xb+KLRXRbenIApygmYopyAKcoJmKKcgCnKCZiinIAp2zlnNBuamJiQ+Y0bNzKzaN5WX18v82jGGs3kmpqaMjN1dWFKKW1sbMg8erbV1VWZDw4OZmaTk5Ny7fHjx2UefTY1f25ra5Nroznm6OiozD98+CBz9f8WzTkLPTqTNydginICpignYIpyAqYoJ2CKcgKmKCdgynbOGc2OonmfmnvduXNHro32Bl65ckXm0RxVPfvly5flWnV9YEopVVVVyXxgYEDmar9otMdWHfmZUnykqLr+MDoKNfreY2NjMo/mnNH3V5hzArsM5QRMUU7AFOUETFFOwBTlBExRTsBUTs0TKyoq5LAxmkWWUjHXrjU2Nsq1XV1dMr927ZrMo32L1dXVmVlLS4tce+DAAZm/e/dO5pubmzJX8zz13Cml1N/fL/Pp6WmZq7OGHz9+LNfevn1b5tF8ODoPeGVlRebF2N7e/uMvM29OwBTlBExRTsAU5QRMUU7AFOUETMlRSi6X27lZSaDQbTgpxduq6urqZN7T0yPzvr4+mavjJ6NxRHR9YWdnp8yjkcTU1FRmVsxWuJTiccS9e/cys2hEND4+LvO1tTWZb21tyVxtZysWoxSgzFBOwBTlBExRTsAU5QRMUU7AFOUETJXtnDOirsKL5pyR6IjI6ArBmpqazCzaEjY8PCzzaF63uLgo89bW1sxsYWFBrlUz0pTi//e5ubnMLNputr6+LvOd3N4YYc4JlBnKCZiinIApygmYopyAKcoJmKKcgKldO+esrKwsKEspnolFR0RGs0Y154yerampSeaRaM9lMftka2trZR4dT6meLZpj5vN5mUefayfnoMw5gTJDOQFTlBMwRTkBU5QTMEU5AVOUEzBVtlcARoqZ10WzxmhfYjRLVF8/mpFGn6uYGWtK+tmLfbZifl+in0kpz5UtNeacQJmhnIApygmYopyAKcoJmKKcgCnKCZjatfs58Xdx3q8ZYc4JlBnKCZiinIApygmYopyAKcoJmMq+Jw8oI86jkkLx5gRMUU7AFOUETFFOwBTlBExRTsAU5QRMMefEf6aY40iL5TznLPT/hTcnYIpyAqYoJ2CKcgKmKCdginICpignYEoejQlg5/DmBExRTsAU5QRMUU7AFOUETFFOwNQ/vO8czHOLoLIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # decoder as a generative model\n",
    "    import matplotlib.pyplot as plt\n",
    "    decoder = ae.decoder\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        # generate a tensor of random noise with size 1, feature_dim\n",
    "        x_in = torch.randn(1, args.feature_dim)\n",
    "        x_tilde = decoder.forward(x_in)\n",
    "        plt.imshow(x_tilde[0].detach().numpy().reshape(28, 28), cmap=\"gray\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52772734322c44a04e342c358be70f2ff4d97da358cb3cd38ceb0f6066598be5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
