{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Biases (`wandb`) Demo\n",
    "\n",
    "In deep learning, we perform a lot of model training especially for novel neural architectures. The problem is deep learning frameworks like PyTorch do not provide sufficient tools to visualize input data, track the progress of our experiments, log data, and visualize the outputs. \n",
    "\n",
    "`wandb` addresses this problem. In this demo, we show how to use `wandb` to visualize input data, prediction, and training progress in the form value of loss function and validation accuracy. \n",
    "\n",
    "**Note**: Before running this demo, please make sure that you have `wandb.ai` free account. \n",
    "\n",
    "Let us install `wandb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import** the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import wandb\n",
    "import datetime\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from ui import progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Login to and initialize** `wandb`. You will need to use your `wandb` API to run this demo.\n",
    "\n",
    "As the config indicates, we will train our model using `cifar10` dataset, learning rate of `0.1`, and batch size of `128` for `100` epochs. \n",
    "\n",
    "epochs means a complete sampling of the dataset (train). In the `wandb` plots, step is the term used instead of epoch.  \n",
    "batch size is the number of samples per training step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrowel\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2022-03-10 18:31:28.258271: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-10 18:31:28.258315: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rowel/github/roatienza/Deep-Learning-Experiments/versions/2022/tools/python/wandb/run-20220310_183126-27yawtsq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/upeee/wandb-project/runs/27yawtsq\" target=\"_blank\">glad-dream-31</a></strong> to <a href=\"https://wandb.ai/upeee/wandb-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'epochs': 100, 'batch_size': 128, 'dataset': 'cifar10'}\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "config = {\n",
    "  \"learning_rate\": 0.1,\n",
    "  \"epochs\": 100,\n",
    "  \"batch_size\": 128,\n",
    "  \"dataset\": \"cifar10\"\n",
    "}\n",
    "run = wandb.init(project=\"wandb-project\", entity=\"upeee\", config=config)\n",
    "\n",
    "print(wandb.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "Use a ResNet18 from `torchvision`. Remove the last layer that was used for 1k-class ImageNet classification. Since we will use CIFAR10, the last layer is replaced by a linear layer with 10 outputs. We will train the model from scratch, so we set `pretrained=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torchvision.models.resnet18(pretrained=False, progress=True)\n",
    "\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 10)  \n",
    "model.to(device)\n",
    "\n",
    "# watch model gradients during training\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function, Optimizer, Scheduler and DataLoader\n",
    "\n",
    "The appropriate loss function is cross entropy for multi-category classfication. We use `SGD` or stochastic gradient descent for optimization. Our learning rate that starts at `0.1` decays to zero at the end of total number of epochs. The decay is controlled by a cosine learning rate decay scheduler. \n",
    "\n",
    "Finally, we use `cifar10` dataset that is available in `torchvision`. We will discuss datasets and dataloaders in our future demo. For the meantime, we can treat dataloader as a data strcuture that dispenses batch size data from either the train or test split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=wandb.config.learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=wandb.config.epochs)\n",
    "\n",
    "x_train = datasets.CIFAR10(root='./data', train=True, \n",
    "                           download=True, \n",
    "                           transform=transforms.ToTensor())\n",
    "x_test = datasets.CIFAR10(root='./data',\n",
    "                          train=False, \n",
    "                          download=True, \n",
    "                          transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(x_train, \n",
    "                          batch_size=wandb.config.batch_size, \n",
    "                          shuffle=True, \n",
    "                          num_workers=2)\n",
    "test_loader = DataLoader(x_test, \n",
    "                         batch_size=wandb.config.batch_size, \n",
    "                         shuffle=False, \n",
    "                         num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visulaizing sample data from test split\n",
    "\n",
    "We can visualize data from the test split by getting a batch sample: `image, label = iter(test_loader).next()`. We use `wandb` table to create a column for image, grount truth label and initial model predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat vs.  dog\n",
      "ship vs.  frog\n",
      "ship vs.  deer\n",
      "airplane vs.  horse\n",
      "frog vs.  frog\n",
      "frog vs.  deer\n",
      "automobile vs.  deer\n",
      "frog vs.  deer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_human = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "table_test = wandb.Table(columns=['Image', \"Ground Truth\", \"Initial Pred Label\",])\n",
    "\n",
    "image, label = iter(test_loader).next()\n",
    "with torch.no_grad():\n",
    "  pred = torch.argmax(model(image.to(device)), dim=1).cpu().numpy()\n",
    "\n",
    "for i in range(8):\n",
    "  table_test.add_data(wandb.Image(image[i]),\n",
    "                      label_human[label[i]], \n",
    "                      label_human[pred[i]])\n",
    "  print(label_human[label[i]], \"vs. \",  label_human[pred[i]])\n",
    "\n",
    "#wandb.log({\"Test data\": table_test})\n",
    "#wandb.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The train loop\n",
    "\n",
    "At every epoch, we will run the train loop for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  model.train()\n",
    "  train_loss = 0\n",
    "  correct = 0\n",
    "  train_samples = 0\n",
    "\n",
    "  # sample a batch. compute loss and backpropagate\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    target = target.to(device)\n",
    "    output = model(data.to(device))\n",
    "    loss_value = loss(output, target)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step(epoch)\n",
    "    train_loss += loss_value.item()\n",
    "    train_samples += len(data)\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    if batch_idx % 10 == 0:\n",
    "      accuracy = 100. * correct / len(train_loader.dataset)\n",
    "      progress_bar(batch_idx,\n",
    "                   len(train_loader),\n",
    "                  'Train Epoch: {} \\tLoss: {:.6f}\\tAcc: {:.2f}%%'.format(epoch+1, \n",
    "                  train_loss/train_samples, accuracy))\n",
    "  \n",
    "  train_loss /= len(train_loader.dataset)\n",
    "  accuracy = 100. * correct / len(train_loader.dataset)\n",
    "\n",
    "  return accuracy, train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The validation loop\n",
    "\n",
    "After every epoch, we will run the validation loop for the model. In this way, we can track the progress of our model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = model(data.to(device))   \n",
    "      target = target.to(device)\n",
    "\n",
    "      test_loss += loss(output, target).item()\n",
    "      pred = output.argmax(dim=1, keepdim=True)\n",
    "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "  print('\\nTest Loss: {:.4f}, Acc: {:.2f}%\\n'.format(test_loss, accuracy))\n",
    "\n",
    "  return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `wandb` plots\n",
    "\n",
    "Finally, we will use `wandb` to visualize the training progress. We will use the following plots:\n",
    "- Model gradients (`wandb.watch(model)`)\n",
    "- Training loss (`\"train loss\": train_loss,`)\n",
    "- Validation accuracy (`\"Test accuracy\": accuracy,`)\n",
    "- Learning rate which decreases over epochs (`\"Learning rate\": optimizer.param_groups[0]['lr']`)\n",
    "\n",
    "We re-use the earlier `table_test` to see the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rowel/anaconda3/lib/python3.7/site-packages/IPython/core/display.py:724: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/upeee/wandb-project/runs/27yawtsq?jupyter=true\" style=\"border:none;width:100%;height:720px;\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [>.............................]  Step: 39s160ms | Tot: 1ms | Train Epoch: 1 \tLoss: 0.019931\tAcc: 0.03%%\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 1/391 \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rowel/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>]  Step: 165ms | Tot: 6s535ms | Train Epoch: 1 \tLoss: 0.012874\tAcc: 44.16% 391/391 \n",
      "\n",
      "Test Loss: 0.0124, Acc: 43.66%\n",
      "\n",
      " [=============================>]  Step: 160ms | Tot: 6s550ms | Train Epoch: 2 \tLoss: 0.009041\tAcc: 59.58% 391/391 \n",
      "\n",
      "Test Loss: 0.0101, Acc: 58.50%\n",
      "\n",
      " [=============================>]  Step: 160ms | Tot: 6s603ms | Train Epoch: 3 \tLoss: 0.007251\tAcc: 67.72% 391/391 \n",
      "\n",
      "Test Loss: 0.0115, Acc: 50.54%\n",
      "\n",
      " [=============================>]  Step: 159ms | Tot: 6s583ms | Train Epoch: 4 \tLoss: 0.006025\tAcc: 73.05% 391/391 \n",
      "\n",
      "Test Loss: 0.0109, Acc: 55.42%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s531ms | Train Epoch: 5 \tLoss: 0.005074\tAcc: 77.29% 391/391 \n",
      "\n",
      "Test Loss: 0.0089, Acc: 62.43%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s615ms | Train Epoch: 6 \tLoss: 0.004251\tAcc: 80.78% 391/391 \n",
      "\n",
      "Test Loss: 0.0095, Acc: 62.93%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s548ms | Train Epoch: 7 \tLoss: 0.003511\tAcc: 84.27% 391/391 \n",
      "\n",
      "Test Loss: 0.0089, Acc: 64.70%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s625ms | Train Epoch: 8 \tLoss: 0.002836\tAcc: 87.17% 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 53.25%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s570ms | Train Epoch: 9 \tLoss: 0.002340\tAcc: 89.34% 391/391 \n",
      "\n",
      "Test Loss: 0.0100, Acc: 64.97%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s616ms | Train Epoch: 10 \tLoss: 0.001837\tAcc: 91.61% 391/391 \n",
      "\n",
      "Test Loss: 0.0115, Acc: 64.31%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s659ms | Train Epoch: 11 \tLoss: 0.001573\tAcc: 92.91% 391/391 \n",
      "\n",
      "Test Loss: 0.0104, Acc: 69.05%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s601ms | Train Epoch: 12 \tLoss: 0.001279\tAcc: 94.22% 391/391 \n",
      "\n",
      "Test Loss: 0.0159, Acc: 60.74%\n",
      "\n",
      " [=============================>]  Step: 160ms | Tot: 6s581ms | Train Epoch: 13 \tLoss: 0.001042\tAcc: 95.20% 391/391 \n",
      "\n",
      "Test Loss: 0.0105, Acc: 70.72%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s711ms | Train Epoch: 14 \tLoss: 0.000882\tAcc: 96.04% 391/391 \n",
      "\n",
      "Test Loss: 0.0125, Acc: 67.33%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s574ms | Train Epoch: 15 \tLoss: 0.000702\tAcc: 96.80% 391/391 \n",
      "\n",
      "Test Loss: 0.0120, Acc: 71.09%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s727ms | Train Epoch: 16 \tLoss: 0.000638\tAcc: 97.07% 391/391 \n",
      "\n",
      "Test Loss: 0.0124, Acc: 69.40%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s619ms | Train Epoch: 17 \tLoss: 0.000549\tAcc: 97.61% 391/391 \n",
      "\n",
      "Test Loss: 0.0120, Acc: 71.47%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s669ms | Train Epoch: 18 \tLoss: 0.000445\tAcc: 98.05% 391/391 \n",
      "\n",
      "Test Loss: 0.0137, Acc: 67.95%\n",
      "\n",
      " [=============================>]  Step: 160ms | Tot: 6s584ms | Train Epoch: 19 \tLoss: 0.000373\tAcc: 98.36% 391/391 \n",
      "\n",
      "Test Loss: 0.0128, Acc: 70.63%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s646ms | Train Epoch: 20 \tLoss: 0.000370\tAcc: 98.33% 391/391 \n",
      "\n",
      "Test Loss: 0.0128, Acc: 72.02%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s705ms | Train Epoch: 21 \tLoss: 0.000343\tAcc: 98.45% 391/391 \n",
      "\n",
      "Test Loss: 0.0124, Acc: 72.00%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s633ms | Train Epoch: 22 \tLoss: 0.000247\tAcc: 98.93% 391/391 \n",
      "\n",
      "Test Loss: 0.0136, Acc: 71.94%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s731ms | Train Epoch: 23 \tLoss: 0.000192\tAcc: 99.18% 391/391 \n",
      "\n",
      "Test Loss: 0.0144, Acc: 71.65%\n",
      "\n",
      " [==========================>...]  Step: 167ms | Tot: 5s873ms | Train Epoch: 24 \tLoss: 0.000200\tAcc: 86.57% 341/391 \r"
     ]
    }
   ],
   "source": [
    "run.display(height=720)\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "for epoch in range(wandb.config[\"epochs\"]):\n",
    "    _, train_loss = train(epoch)\n",
    "    accuracy, _ = test()\n",
    "    wandb.log({\n",
    "        \"Test accuracy\": accuracy,\n",
    "        \"Train loss\": train_loss,\n",
    "        \"Learning rate\": optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "\n",
    "elapsed_time = datetime.datetime.now() - start_time\n",
    "print(\"Elapsed time: %s\" % elapsed_time)\n",
    "wandb.log({\"Elapsed training time\": elapsed_time}) \n",
    "    \n",
    "with torch.no_grad():\n",
    "  pred = torch.argmax(model(image.to(device)), dim=1).cpu().numpy()\n",
    "\n",
    "final_pred = []\n",
    "for i in range(8):\n",
    "    final_pred.append(label_human[pred[i]])\n",
    "    print(label_human[label[i]], \"vs. \",  final_pred[i])\n",
    "\n",
    "table_test.add_column(name=\"Final Pred Label\", data=final_pred)\n",
    "\n",
    "wandb.log({\"Test data\": table_test})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52772734322c44a04e342c358be70f2ff4d97da358cb3cd38ceb0f6066598be5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
