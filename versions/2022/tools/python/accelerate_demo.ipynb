{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Accelerate Demo\n",
    "\n",
    "**Note**: Before running this demo, please make sure that you have `wandb.ai` free account. \n",
    "\n",
    "Let us install `accelerate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/rowel/anaconda3/lib/python3.7/site-packages (0.5.1)\r\n",
      "Requirement already satisfied: pyyaml in /home/rowel/anaconda3/lib/python3.7/site-packages (from accelerate) (6.0)\r\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/rowel/anaconda3/lib/python3.7/site-packages (from accelerate) (1.10.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/rowel/anaconda3/lib/python3.7/site-packages (from accelerate) (1.21.2)\r\n",
      "Requirement already satisfied: typing-extensions in /home/rowel/anaconda3/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (3.10.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import** the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import wandb\n",
    "import datetime\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from ui import progress_bar\n",
    "\n",
    "# This is a demo of the PyTorch Accelerate API.\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`wandb`** initialization. See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrowel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrowel\u001b[0m (\u001b[33mupeee\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rowel/github/roatienza/Deep-Learning-Experiments/versions/2022/tools/python/wandb/run-20221006_085740-1oqmf5kg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/upeee/accelerate-project/runs/1oqmf5kg\" target=\"_blank\">revived-elevator-9</a></strong> to <a href=\"https://wandb.ai/upeee/accelerate-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "config = {\n",
    "  \"learning_rate\": 0.1,\n",
    "  \"epochs\": 100,\n",
    "  \"batch_size\": 128,\n",
    "  \"dataset\": \"cifar10\"\n",
    "}\n",
    "run = wandb.init(project=\"accelerate-project\", entity=\"upeee\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "Use a ResNet18 from `torchvision`. See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rowel/anaconda3/envs/voice/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rowel/anaconda3/envs/voice/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the code to be replaced with the Accelerate API.\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=False, progress=True)\n",
    "\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 10) \n",
    "\n",
    "# Replace the model with the Accelerate API.\n",
    "#model.to(device)\n",
    "\n",
    "# watch model gradients during training\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function, Optimizer, Scheduler and DataLoader\n",
    "\n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=wandb.config.learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=wandb.config.epochs)\n",
    "\n",
    "x_train = datasets.CIFAR10(root='./data', train=True, \n",
    "                           download=True, \n",
    "                           transform=transforms.ToTensor())\n",
    "x_test = datasets.CIFAR10(root='./data',\n",
    "                          train=False, \n",
    "                          download=True, \n",
    "                          transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(x_train,\n",
    "                          batch_size=wandb.config.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "test_loader = DataLoader(x_test, \n",
    "                         batch_size=wandb.config.batch_size, \n",
    "                         shuffle=False, \n",
    "                         num_workers=2)\n",
    "\n",
    "# Accelerate API\n",
    "model = accelerator.prepare(model)\n",
    "optimizer = accelerator.prepare(optimizer)\n",
    "scheduler = accelerator.prepare(scheduler)\n",
    "train_loader = accelerator.prepare(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visulaizing sample data from test split\n",
    "\n",
    "\n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details.\n",
    "\n",
    "\n",
    "Note the last line that uses Accelerate API to wrap the model, optimizer, data loaders and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat vs.  automobile\n",
      "ship vs.  automobile\n",
      "ship vs.  automobile\n",
      "airplane vs.  automobile\n",
      "frog vs.  automobile\n",
      "frog vs.  automobile\n",
      "automobile vs.  automobile\n",
      "frog vs.  automobile\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_human = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "table_test = wandb.Table(columns=['Image', \"Ground Truth\", \"Initial Pred Label\",])\n",
    "\n",
    "image, label = iter(test_loader).next()\n",
    "test_loader = accelerator.prepare(test_loader)\n",
    "image = image.to(accelerator.device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  pred = torch.argmax(model(image), dim=1).cpu().numpy()\n",
    "\n",
    "for i in range(8):\n",
    "  table_test.add_data(wandb.Image(image[i]),\n",
    "                      label_human[label[i]], \n",
    "                      label_human[pred[i]])\n",
    "  print(label_human[label[i]], \"vs. \",  label_human[pred[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The train loop\n",
    "\n",
    "Using Accelerate, we do not need to transfer the model to the `device`.\n",
    "\n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  model.train()\n",
    "  train_loss = 0\n",
    "  correct = 0\n",
    "  train_samples = 0\n",
    "\n",
    "  # sample a batch. compute loss and backpropagate\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    # Replaced by the Accelerate API.\n",
    "    #target = target.to(device)\n",
    "    #output = model(data.to(device))\n",
    "    \n",
    "    output = model(data)\n",
    "    loss_value = loss(output, target)\n",
    "\n",
    "    # Replaced by the Accelerate API.\n",
    "    #loss_value.backward()\n",
    "    accelerator.backward(loss_value)\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step(epoch)\n",
    "    train_loss += loss_value.item()\n",
    "    train_samples += len(data)\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    if batch_idx % 10 == 0:\n",
    "      accuracy = 100. * correct / len(train_loader.dataset)\n",
    "      progress_bar(batch_idx,\n",
    "                   len(train_loader),\n",
    "                   'Train Epoch: {}, Loss: {:.6f}, Acc: {:.2f}%'.format(epoch+1, \n",
    "                   train_loss/train_samples, accuracy))\n",
    "  \n",
    "  train_loss /= len(train_loader.dataset)\n",
    "  accuracy = 100. * correct / len(train_loader.dataset)\n",
    "\n",
    "  return accuracy, train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The validation loop\n",
    "\n",
    "After every epoch, we will run the validation loop for the model. Again, no need to transfer the data to the `device`.\n",
    "\n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "\n",
    "      # Replaced by the Accelerate API.\n",
    "      #output = model(data.to(device))   \n",
    "      #target = target.to(device)\n",
    "\n",
    "      output = model(data)\n",
    "      test_loss += loss(output, target).item()\n",
    "      pred = output.argmax(dim=1, keepdim=True)\n",
    "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "  print('\\nTest Loss: {:.4f}, Acc: {:.2f}%\\n'.format(test_loss, accuracy))\n",
    "\n",
    "  return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `wandb` plots\n",
    "\n",
    "Finally, we will use `wandb` to visualize the training progress. \n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rowel/anaconda3/envs/voice/lib/python3.9/site-packages/IPython/core/display.py:419: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/upeee/accelerate-project/runs/1oqmf5kg?jupyter=true\" style=\"border:none;width:100%;height:1000px;\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>]  Step: 211ms | Tot: 6s735ms | Train Epoch: 1, Loss: 0.013177, Acc: 42.96 391/391 \n",
      "\n",
      "Test Loss: 0.0165, Acc: 37.32%\n",
      "\n",
      " [=============================>]  Step: 226ms | Tot: 7s152ms | Train Epoch: 2, Loss: 0.009158, Acc: 58.69 391/391 \n",
      "\n",
      "Test Loss: 0.0097, Acc: 56.90%\n",
      "\n",
      " [=============================>]  Step: 236ms | Tot: 7s455ms | Train Epoch: 3, Loss: 0.007397, Acc: 66.54 391/391 ............]  Step: 199ms | Tot: 2s270ms | Train Epoch: 3, Loss: 0.007520, Acc: 22.08 131/391 \n",
      "\n",
      "Test Loss: 0.0097, Acc: 59.42%\n",
      "\n",
      " [=============================>]  Step: 221ms | Tot: 6s994ms | Train Epoch: 4, Loss: 0.006205, Acc: 71.98 391/391 ============>.................]  Step: 201ms | Tot: 2s842ms | Train Epoch: 4, Loss: 0.006190, Acc: 29.71 161/391 \n",
      "\n",
      "Test Loss: 0.0086, Acc: 62.75%\n",
      "\n",
      " [=============================>]  Step: 207ms | Tot: 6s739ms | Train Epoch: 5, Loss: 0.005268, Acc: 76.07 391/391 \n",
      "\n",
      "Test Loss: 0.0093, Acc: 60.71%\n",
      "\n",
      " [=============================>]  Step: 211ms | Tot: 7s265ms | Train Epoch: 6, Loss: 0.004382, Acc: 80.20 391/391 \n",
      "\n",
      "Test Loss: 0.0124, Acc: 53.56%\n",
      "\n",
      " [=============================>]  Step: 205ms | Tot: 6s630ms | Train Epoch: 7, Loss: 0.003617, Acc: 83.74 391/391 \n",
      "\n",
      "Test Loss: 0.0097, Acc: 63.66%\n",
      "\n",
      " [=============================>]  Step: 210ms | Tot: 6s853ms | Train Epoch: 8, Loss: 0.002994, Acc: 86.44 391/391 \n",
      "\n",
      "Test Loss: 0.0105, Acc: 63.38%\n",
      "\n",
      " [=============================>]  Step: 206ms | Tot: 6s712ms | Train Epoch: 9, Loss: 0.002460, Acc: 88.81 391/391 \n",
      "\n",
      "Test Loss: 0.0129, Acc: 59.60%\n",
      "\n",
      " [=============================>]  Step: 205ms | Tot: 6s701ms | Train Epoch: 10, Loss: 0.001956, Acc: 91.07 391/391 \n",
      "\n",
      "Test Loss: 0.0099, Acc: 67.61%\n",
      "\n",
      " [=============================>]  Step: 207ms | Tot: 6s834ms | Train Epoch: 11, Loss: 0.001642, Acc: 92.63 391/391 \n",
      "\n",
      "Test Loss: 0.0124, Acc: 62.41%\n",
      "\n",
      " [=============================>]  Step: 218ms | Tot: 7s111ms | Train Epoch: 12, Loss: 0.001338, Acc: 94.03 391/391 \n",
      "\n",
      "Test Loss: 0.0099, Acc: 69.99%\n",
      "\n",
      " [=============================>]  Step: 216ms | Tot: 6s732ms | Train Epoch: 13, Loss: 0.001120, Acc: 94.98 391/391 \n",
      "\n",
      "Test Loss: 0.0105, Acc: 69.65%\n",
      "\n",
      " [=============================>]  Step: 250ms | Tot: 6s974ms | Train Epoch: 14, Loss: 0.000853, Acc: 96.15 391/391 \n",
      "\n",
      "Test Loss: 0.0109, Acc: 70.90%\n",
      "\n",
      " [=============================>]  Step: 241ms | Tot: 7s196ms | Train Epoch: 15, Loss: 0.000755, Acc: 96.65 391/391 \n",
      "\n",
      "Test Loss: 0.0117, Acc: 69.05%\n",
      "\n",
      " [=============================>]  Step: 212ms | Tot: 6s861ms | Train Epoch: 16, Loss: 0.000638, Acc: 97.16 391/391 \n",
      "\n",
      "Test Loss: 0.0116, Acc: 70.85%\n",
      "\n",
      " [=============================>]  Step: 236ms | Tot: 7s198ms | Train Epoch: 17, Loss: 0.000598, Acc: 97.37 391/391 \n",
      "\n",
      "Test Loss: 0.0114, Acc: 72.14%\n",
      "\n",
      " [=============================>]  Step: 219ms | Tot: 7s178ms | Train Epoch: 18, Loss: 0.000446, Acc: 98.11 391/391 \n",
      "\n",
      "Test Loss: 0.0137, Acc: 69.55%\n",
      "\n",
      " [=============================>]  Step: 211ms | Tot: 6s893ms | Train Epoch: 19, Loss: 0.000425, Acc: 98.14 391/391 \n",
      "\n",
      "Test Loss: 0.0144, Acc: 68.65%\n",
      "\n",
      " [=============================>]  Step: 239ms | Tot: 7s141ms | Train Epoch: 20, Loss: 0.000366, Acc: 98.37 391/391 \n",
      "\n",
      "Test Loss: 0.0186, Acc: 62.15%\n",
      "\n",
      " [=============================>]  Step: 208ms | Tot: 6s777ms | Train Epoch: 21, Loss: 0.000302, Acc: 98.68 391/391 \n",
      "\n",
      "Test Loss: 0.0130, Acc: 71.68%\n",
      "\n",
      " [=============================>]  Step: 213ms | Tot: 6s815ms | Train Epoch: 22, Loss: 0.000277, Acc: 98.72 391/391 \n",
      "\n",
      "Test Loss: 0.0139, Acc: 71.23%\n",
      "\n",
      " [=============================>]  Step: 221ms | Tot: 6s789ms | Train Epoch: 23, Loss: 0.000326, Acc: 98.56 391/391 \n",
      "\n",
      "Test Loss: 0.0127, Acc: 71.52%\n",
      "\n",
      " [=============================>]  Step: 209ms | Tot: 6s978ms | Train Epoch: 24, Loss: 0.000189, Acc: 99.19 391/391 \n",
      "\n",
      "Test Loss: 0.0139, Acc: 71.73%\n",
      "\n",
      " [=============================>]  Step: 222ms | Tot: 6s698ms | Train Epoch: 25, Loss: 0.000193, Acc: 99.13 391/391 \n",
      "\n",
      "Test Loss: 0.0144, Acc: 72.09%\n",
      "\n",
      " [=============================>]  Step: 225ms | Tot: 7s220ms | Train Epoch: 26, Loss: 0.000188, Acc: 99.19 391/391 \n",
      "\n",
      "Test Loss: 0.0156, Acc: 71.47%\n",
      "\n",
      " [=============================>]  Step: 204ms | Tot: 6s767ms | Train Epoch: 27, Loss: 0.000154, Acc: 99.34 391/391 \n",
      "\n",
      "Test Loss: 0.0138, Acc: 72.79%\n",
      "\n",
      " [=============================>]  Step: 211ms | Tot: 6s784ms | Train Epoch: 28, Loss: 0.000096, Acc: 99.60 391/391 \n",
      "\n",
      "Test Loss: 0.0144, Acc: 72.62%\n",
      "\n",
      " [=============================>]  Step: 220ms | Tot: 7s212ms | Train Epoch: 29, Loss: 0.000126, Acc: 99.44 391/391 \n",
      "\n",
      "Test Loss: 0.0143, Acc: 73.00%\n",
      "\n",
      " [=============================>]  Step: 203ms | Tot: 6s615ms | Train Epoch: 30, Loss: 0.000106, Acc: 99.56 391/391 \n",
      "\n",
      "Test Loss: 0.0148, Acc: 71.59%\n",
      "\n",
      " [=============================>]  Step: 208ms | Tot: 7s21ms | Train Epoch: 31, Loss: 0.000095, Acc: 99.58 391/391  \n",
      "\n",
      "Test Loss: 0.0146, Acc: 73.30%\n",
      "\n",
      " [=============================>]  Step: 207ms | Tot: 6s715ms | Train Epoch: 32, Loss: 0.000080, Acc: 99.66 391/391 \n",
      "\n",
      "Test Loss: 0.0145, Acc: 74.16%\n",
      "\n",
      " [=============================>]  Step: 221ms | Tot: 7s154ms | Train Epoch: 33, Loss: 0.000037, Acc: 99.87 391/391 \n",
      "\n",
      "Test Loss: 0.0141, Acc: 74.69%\n",
      "\n",
      " [=============================>]  Step: 294ms | Tot: 7s188ms | Train Epoch: 34, Loss: 0.000025, Acc: 99.90 391/391 \n",
      "\n",
      "Test Loss: 0.0145, Acc: 74.69%\n",
      "\n",
      " [=============================>]  Step: 205ms | Tot: 6s709ms | Train Epoch: 35, Loss: 0.000024, Acc: 99.91 391/391 \n",
      "\n",
      "Test Loss: 0.0155, Acc: 73.71%\n",
      "\n",
      " [=============================>]  Step: 223ms | Tot: 6s943ms | Train Epoch: 36, Loss: 0.000031, Acc: 99.86 391/391 \n",
      "\n",
      "Test Loss: 0.0151, Acc: 74.43%\n",
      "\n",
      " [=============================>]  Step: 202ms | Tot: 6s814ms | Train Epoch: 37, Loss: 0.000012, Acc: 99.95 391/391 \n",
      "\n",
      "Test Loss: 0.0154, Acc: 74.60%\n",
      "\n",
      " [=============================>]  Step: 220ms | Tot: 7s50ms | Train Epoch: 38, Loss: 0.000008, Acc: 99.97 391/391  \n",
      "\n",
      "Test Loss: 0.0155, Acc: 74.64%\n",
      "\n",
      " [=============================>]  Step: 216ms | Tot: 6s694ms | Train Epoch: 39, Loss: 0.000008, Acc: 99.97 391/391 \n",
      "\n",
      "Test Loss: 0.0158, Acc: 74.59%\n",
      "\n",
      " [=============================>]  Step: 228ms | Tot: 6s875ms | Train Epoch: 40, Loss: 0.000009, Acc: 99.96 391/391 \n",
      "\n",
      "Test Loss: 0.0163, Acc: 74.39%\n",
      "\n",
      " [=============================>]  Step: 221ms | Tot: 7s109ms | Train Epoch: 41, Loss: 0.000008, Acc: 99.97 391/391 \n",
      "\n",
      "Test Loss: 0.0160, Acc: 74.68%\n",
      "\n",
      " [=============================>]  Step: 229ms | Tot: 7s139ms | Train Epoch: 42, Loss: 0.000016, Acc: 99.94 391/391 \n",
      "\n",
      "Test Loss: 0.0160, Acc: 74.88%\n",
      "\n",
      " [=============================>]  Step: 205ms | Tot: 6s734ms | Train Epoch: 43, Loss: 0.000007, Acc: 99.98 391/391 \n",
      "\n",
      "Test Loss: 0.0160, Acc: 75.02%\n",
      "\n",
      " [=============================>]  Step: 204ms | Tot: 6s774ms | Train Epoch: 44, Loss: 0.000007, Acc: 99.98 391/391 \n",
      "\n",
      "Test Loss: 0.0163, Acc: 75.03%\n",
      "\n",
      " [=============================>]  Step: 324ms | Tot: 6s760ms | Train Epoch: 45, Loss: 0.000005, Acc: 99.99 391/391 \n",
      "\n",
      "Test Loss: 0.0164, Acc: 75.03%\n",
      "\n",
      " [=============================>]  Step: 221ms | Tot: 7s119ms | Train Epoch: 46, Loss: 0.000002, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0161, Acc: 75.48%\n",
      "\n",
      " [=============================>]  Step: 207ms | Tot: 6s829ms | Train Epoch: 47, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0164, Acc: 75.25%\n",
      "\n",
      " [=============================>]  Step: 205ms | Tot: 7s29ms | Train Epoch: 48, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0166, Acc: 75.50%\n",
      "\n",
      " [=============================>]  Step: 205ms | Tot: 6s749ms | Train Epoch: 49, Loss: 0.000002, Acc: 99.99 391/391 \n",
      "\n",
      "Test Loss: 0.0164, Acc: 75.51%\n",
      "\n",
      " [=============================>]  Step: 232ms | Tot: 7s121ms | Train Epoch: 50, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0166, Acc: 75.40%\n",
      "\n",
      " [=============================>]  Step: 207ms | Tot: 6s745ms | Train Epoch: 51, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.46%\n",
      "\n",
      " [=============================>]  Step: 225ms | Tot: 6s978ms | Train Epoch: 52, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0166, Acc: 75.44%\n",
      "\n",
      " [=============================>]  Step: 227ms | Tot: 7s53ms | Train Epoch: 53, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0166, Acc: 75.66%\n",
      "\n",
      " [=============================>]  Step: 218ms | Tot: 7s200ms | Train Epoch: 54, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.73%\n",
      "\n",
      " [=============================>]  Step: 220ms | Tot: 7s101ms | Train Epoch: 55, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0166, Acc: 75.71%\n",
      "\n",
      " [=============================>]  Step: 212ms | Tot: 6s700ms | Train Epoch: 56, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0165, Acc: 75.70%\n",
      "\n",
      " [=============================>]  Step: 201ms | Tot: 6s791ms | Train Epoch: 57, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.80%\n",
      "\n",
      " [=============================>]  Step: 208ms | Tot: 6s722ms | Train Epoch: 58, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.66%\n",
      "\n",
      " [=============================>]  Step: 210ms | Tot: 6s918ms | Train Epoch: 59, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.62%\n",
      "\n",
      " [=============================>]  Step: 224ms | Tot: 7s161ms | Train Epoch: 60, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0166, Acc: 75.54%\n",
      "\n",
      " [=============================>]  Step: 213ms | Tot: 6s757ms | Train Epoch: 61, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.47%\n",
      "\n",
      " [=============================>]  Step: 211ms | Tot: 7s171ms | Train Epoch: 62, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.89%\n",
      "\n",
      " [=============================>]  Step: 217ms | Tot: 7s78ms | Train Epoch: 63, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.72%\n",
      "\n",
      " [=============================>]  Step: 210ms | Tot: 6s674ms | Train Epoch: 64, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.62%\n",
      "\n",
      " [=============================>]  Step: 221ms | Tot: 7s208ms | Train Epoch: 65, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.58%\n",
      "\n",
      " [=============================>]  Step: 221ms | Tot: 7s131ms | Train Epoch: 66, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.74%\n",
      "\n",
      " [=============================>]  Step: 340ms | Tot: 7s417ms | Train Epoch: 67, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.50%\n",
      "\n",
      " [=============================>]  Step: 218ms | Tot: 7s160ms | Train Epoch: 68, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.75%\n",
      "\n",
      " [=============================>]  Step: 209ms | Tot: 6s843ms | Train Epoch: 69, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.68%\n",
      "\n",
      " [=============================>]  Step: 209ms | Tot: 6s837ms | Train Epoch: 70, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.58%\n",
      "\n",
      " [=============================>]  Step: 325ms | Tot: 6s796ms | Train Epoch: 71, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.70%\n",
      "\n",
      " [=============================>]  Step: 207ms | Tot: 6s812ms | Train Epoch: 72, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.72%\n",
      "\n",
      " [=============================>]  Step: 216ms | Tot: 6s926ms | Train Epoch: 73, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.56%\n",
      "\n",
      " [=============================>]  Step: 259ms | Tot: 6s841ms | Train Epoch: 74, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.68%\n",
      "\n",
      " [=============================>]  Step: 222ms | Tot: 7s156ms | Train Epoch: 75, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.60%\n",
      "\n",
      " [=============================>]  Step: 207ms | Tot: 6s825ms | Train Epoch: 76, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.66%\n",
      "\n",
      " [=============================>]  Step: 204ms | Tot: 6s761ms | Train Epoch: 77, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.77%\n",
      "\n",
      " [=============================>]  Step: 206ms | Tot: 6s754ms | Train Epoch: 78, Loss: 0.000000, Acc: 100.00 391/391 1/391 \n",
      "\n",
      "Test Loss: 0.0165, Acc: 75.57%\n",
      "\n",
      " [=============================>]  Step: 212ms | Tot: 6s731ms | Train Epoch: 79, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.72%\n",
      "\n",
      " [=============================>]  Step: 206ms | Tot: 6s812ms | Train Epoch: 80, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.65%\n",
      "\n",
      " [=============================>]  Step: 244ms | Tot: 6s742ms | Train Epoch: 81, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.60%\n",
      "\n",
      " [=============================>]  Step: 230ms | Tot: 7s210ms | Train Epoch: 82, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.66%\n",
      "\n",
      " [=============================>]  Step: 299ms | Tot: 6s854ms | Train Epoch: 83, Loss: 0.000000, Acc: 100.00 391/391  \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.66%\n",
      "\n",
      " [=============================>]  Step: 215ms | Tot: 7s131ms | Train Epoch: 84, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.64%\n",
      "\n",
      " [=============================>]  Step: 217ms | Tot: 6s737ms | Train Epoch: 85, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.68%\n",
      "\n",
      " [=============================>]  Step: 207ms | Tot: 6s782ms | Train Epoch: 86, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.56%\n",
      "\n",
      " [=============================>]  Step: 208ms | Tot: 6s844ms | Train Epoch: 87, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.67%\n",
      "\n",
      " [=============================>]  Step: 288ms | Tot: 6s806ms | Train Epoch: 88, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.75%\n",
      "\n",
      " [=============================>]  Step: 268ms | Tot: 7s141ms | Train Epoch: 89, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.66%\n",
      "\n",
      " [=============================>]  Step: 323ms | Tot: 6s890ms | Train Epoch: 90, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.71%\n",
      "\n",
      " [=============================>]  Step: 208ms | Tot: 6s812ms | Train Epoch: 91, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.58%\n",
      "\n",
      " [=============================>]  Step: 218ms | Tot: 6s809ms | Train Epoch: 92, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.55%\n",
      "\n",
      " [=============================>]  Step: 212ms | Tot: 6s827ms | Train Epoch: 93, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.60%\n",
      "\n",
      " [=============================>]  Step: 220ms | Tot: 6s795ms | Train Epoch: 94, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.62%\n",
      "\n",
      " [=============================>]  Step: 216ms | Tot: 6s785ms | Train Epoch: 95, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.60%\n",
      "\n",
      " [=============================>]  Step: 209ms | Tot: 6s791ms | Train Epoch: 96, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.66%\n",
      "\n",
      " [=============================>]  Step: 203ms | Tot: 6s721ms | Train Epoch: 97, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.50%\n",
      "\n",
      " [=============================>]  Step: 227ms | Tot: 7s6ms | Train Epoch: 98, Loss: 0.000000, Acc: 100.00 391/391  \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.75%\n",
      "\n",
      " [=============================>]  Step: 221ms | Tot: 7s98ms | Train Epoch: 99, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.69%\n",
      "\n",
      " [=============================>]  Step: 300ms | Tot: 6s932ms | Train Epoch: 100, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.69%\n",
      "\n",
      "Elapsed time: 0:13:32.687888\n",
      "cat vs.  cat\n",
      "ship vs.  ship\n",
      "ship vs.  ship\n",
      "airplane vs.  airplane\n",
      "frog vs.  frog\n",
      "frog vs.  frog\n",
      "automobile vs.  truck\n",
      "frog vs.  frog\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning rate</td><td>████████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Test accuracy</td><td>▁▅▄▆▆▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>Test loss</td><td>▇▁▄▂▄▂▃▅▄▄▇▅▆▆▆▇▇▇▇▇██▇█████████████████</td></tr><tr><td>Train accuracy</td><td>▁▄▆▆▇▇██████████████████████████████████</td></tr><tr><td>Train loss</td><td>█▅▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best accuracy</td><td>75.89</td></tr><tr><td>Elapsed train time</td><td>0:13:32.687888</td></tr><tr><td>Learning rate</td><td>2e-05</td></tr><tr><td>Test accuracy</td><td>75.69</td></tr><tr><td>Test loss</td><td>0.01694</td></tr><tr><td>Train accuracy</td><td>100.0</td></tr><tr><td>Train loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">revived-elevator-9</strong>: <a href=\"https://wandb.ai/upeee/accelerate-project/runs/1oqmf5kg\" target=\"_blank\">https://wandb.ai/upeee/accelerate-project/runs/1oqmf5kg</a><br/>Synced 7 W&B file(s), 1 media file(s), 9 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221006_085740-1oqmf5kg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.display(height=1000)\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "best_acc = 0\n",
    "for epoch in range(wandb.config[\"epochs\"]):\n",
    "    train_acc, train_loss = train(epoch)\n",
    "    test_acc, test_loss = test()\n",
    "    if test_acc > best_acc:\n",
    "        wandb.run.summary[\"Best accuracy\"] = test_acc\n",
    "        best_acc = test_acc\n",
    "        accelerator.save(model, \"resnet18_best_acc.pth\")\n",
    "    wandb.log({\n",
    "        \"Train accuracy\": train_acc,\n",
    "        \"Test accuracy\": test_acc,\n",
    "        \"Train loss\": train_loss,\n",
    "        \"Test loss\": test_loss,\n",
    "        \"Learning rate\": optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "\n",
    "elapsed_time = datetime.datetime.now() - start_time\n",
    "print(\"Elapsed time: %s\" % elapsed_time)\n",
    "wandb.run.summary[\"Elapsed train time\"] = str(elapsed_time)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  pred = torch.argmax(model(image), dim=1).cpu().numpy()\n",
    "\n",
    "final_pred = []\n",
    "for i in range(8):\n",
    "    final_pred.append(label_human[pred[i]])\n",
    "    print(label_human[label[i]], \"vs. \",  final_pred[i])\n",
    "\n",
    "table_test.add_column(name=\"Final Pred Label\", data=final_pred)\n",
    "\n",
    "wandb.log({\"Test data\": table_test})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the best performing model\n",
    "\n",
    "In the following code, we load the best performing model. The model is saved in `./resnet18_best_acc.pth`. The average accuracy of the model is the same as the one in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.0168, Acc: 75.89%\n",
      "\n",
      "Best accuracy: 75.89\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"resnet18_best_acc.pth\")\n",
    "# Using Accelerator API\n",
    "model = accelerator.prepare(model)\n",
    "accuracy, _ = test()\n",
    "print(\"Best accuracy: %.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('voice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "975196f818cef4d8418ca9caaf131be767e393cdee268c01707e7bb8d73c6de2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
