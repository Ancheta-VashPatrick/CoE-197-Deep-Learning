{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Accelerate Demo\n",
    "\n",
    "**Note**: Before running this demo, please make sure that you have `wandb.ai` free account. \n",
    "\n",
    "Let us install `accelerate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 669 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/rowel/anaconda3/lib/python3.7/site-packages (from accelerate) (1.21.2)\n",
      "Requirement already satisfied: pyyaml in /home/rowel/anaconda3/lib/python3.7/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/rowel/anaconda3/lib/python3.7/site-packages (from accelerate) (1.10.2)\n",
      "Requirement already satisfied: typing-extensions in /home/rowel/anaconda3/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (3.10.0.2)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import** the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import wandb\n",
    "import datetime\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from ui import progress_bar\n",
    "\n",
    "# This is a demo of the PyTorch Accelerate API.\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`wandb`** initialization. See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrowel\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2022-03-11 10:00:23.760100: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-11 10:00:23.760140: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rowel/github/roatienza/Deep-Learning-Experiments/versions/2022/tools/python/wandb/run-20220311_100021-3cgm4xqc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/upeee/accelerate-project/runs/3cgm4xqc\" target=\"_blank\">zany-silence-1</a></strong> to <a href=\"https://wandb.ai/upeee/accelerate-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "config = {\n",
    "  \"learning_rate\": 0.1,\n",
    "  \"epochs\": 100,\n",
    "  \"batch_size\": 128,\n",
    "  \"dataset\": \"cifar10\"\n",
    "}\n",
    "run = wandb.init(project=\"accelerate-project\", entity=\"upeee\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "Use a ResNet18 from `torchvision`. See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the code to be replaced with the Accelerate API.\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=False, progress=True)\n",
    "\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 10) \n",
    "\n",
    "# Replace the model with the Accelerate API.\n",
    "#model.to(device)\n",
    "\n",
    "# watch model gradients during training\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function, Optimizer, Scheduler and DataLoader\n",
    "\n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=wandb.config.learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=wandb.config.epochs)\n",
    "\n",
    "x_train = datasets.CIFAR10(root='./data', train=True, \n",
    "                           download=True, \n",
    "                           transform=transforms.ToTensor())\n",
    "x_test = datasets.CIFAR10(root='./data',\n",
    "                          train=False, \n",
    "                          download=True, \n",
    "                          transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(x_train, \n",
    "                          batch_size=wandb.config.batch_size, \n",
    "                          shuffle=True, \n",
    "                          num_workers=2)\n",
    "test_loader = DataLoader(x_test, \n",
    "                         batch_size=wandb.config.batch_size, \n",
    "                         shuffle=False, \n",
    "                         num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visulaizing sample data from test split\n",
    "\n",
    "\n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details.\n",
    "\n",
    "\n",
    "Note the last line that uses Accelerate API to wrap the model, optimizer, data loaders and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat vs.  deer\n",
      "ship vs.  deer\n",
      "ship vs.  deer\n",
      "airplane vs.  deer\n",
      "frog vs.  deer\n",
      "frog vs.  deer\n",
      "automobile vs.  deer\n",
      "frog vs.  deer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_human = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "table_test = wandb.Table(columns=['Image', \"Ground Truth\", \"Initial Pred Label\",])\n",
    "\n",
    "image, label = iter(test_loader).next()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  pred = torch.argmax(model(image), dim=1).cpu().numpy()\n",
    "  # Replace the model with the Accelerate API.\n",
    "  #pred = torch.argmax(model(image.to(device)), dim=1).cpu().numpy()\n",
    "\n",
    "for i in range(8):\n",
    "  table_test.add_data(wandb.Image(image[i]),\n",
    "                      label_human[label[i]], \n",
    "                      label_human[pred[i]])\n",
    "  print(label_human[label[i]], \"vs. \",  label_human[pred[i]])\n",
    "\n",
    "# Accelerate API\n",
    "model, optimizer, scheduler, train_loader, test_loader = accelerator.prepare(model,\n",
    "                                                                             optimizer,\n",
    "                                                                             scheduler, \n",
    "                                                                             train_loader, \n",
    "                                                                             test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The train loop\n",
    "\n",
    "Using Accelerate, we do not need to transfer the model to the `device`.\n",
    "\n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  model.train()\n",
    "  train_loss = 0\n",
    "  correct = 0\n",
    "  train_samples = 0\n",
    "\n",
    "  # sample a batch. compute loss and backpropagate\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    # Replaced by the Accelerate API.\n",
    "    #target = target.to(device)\n",
    "    #output = model(data.to(device))\n",
    "    \n",
    "    output = model(data)\n",
    "    loss_value = loss(output, target)\n",
    "\n",
    "    # Replaced by the Accelerate API.\n",
    "    #loss_value.backward()\n",
    "    accelerator.backward(loss_value)\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step(epoch)\n",
    "    train_loss += loss_value.item()\n",
    "    train_samples += len(data)\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    if batch_idx % 10 == 0:\n",
    "      accuracy = 100. * correct / len(train_loader.dataset)\n",
    "      progress_bar(batch_idx,\n",
    "                   len(train_loader),\n",
    "                   'Train Epoch: {}, Loss: {:.6f}, Acc: {:.2f}%'.format(epoch+1, \n",
    "                   train_loss/train_samples, accuracy))\n",
    "  \n",
    "  train_loss /= len(train_loader.dataset)\n",
    "  accuracy = 100. * correct / len(train_loader.dataset)\n",
    "\n",
    "  return accuracy, train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The validation loop\n",
    "\n",
    "After every epoch, we will run the validation loop for the model. Again, no need to transfer the data to the `device`.\n",
    "\n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "\n",
    "      # Replaced by the Accelerate API.\n",
    "      #output = model(data.to(device))   \n",
    "      #target = target.to(device)\n",
    "\n",
    "      output = model(data)\n",
    "      test_loss += loss(output, target).item()\n",
    "      pred = output.argmax(dim=1, keepdim=True)\n",
    "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "  print('\\nTest Loss: {:.4f}, Acc: {:.2f}%\\n'.format(test_loss, accuracy))\n",
    "\n",
    "  return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `wandb` plots\n",
    "\n",
    "Finally, we will use `wandb` to visualize the training progress. \n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rowel/anaconda3/lib/python3.7/site-packages/IPython/core/display.py:724: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/upeee/accelerate-project/runs/3cgm4xqc?jupyter=true\" style=\"border:none;width:100%;height:1000px;\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [>.............................]  Step: 2m32s | Tot: 0ms | Train Epoch: 1, Loss: 0.019245, Acc: 0.02%\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 1/391 \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rowel/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>]  Step: 163ms | Tot: 6s656ms | Train Epoch: 1, Loss: 0.012896, Acc: 43.67 391/391 \n",
      "\n",
      "Test Loss: 0.0180, Acc: 38.62%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s614ms | Train Epoch: 2, Loss: 0.008939, Acc: 59.61 391/391 \n",
      "\n",
      "Test Loss: 0.0100, Acc: 55.81%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s727ms | Train Epoch: 3, Loss: 0.007248, Acc: 67.52 391/391 \n",
      "\n",
      "Test Loss: 0.0106, Acc: 54.48%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s699ms | Train Epoch: 4, Loss: 0.006077, Acc: 72.66 391/391 \n",
      "\n",
      "Test Loss: 0.0141, Acc: 48.35%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s738ms | Train Epoch: 5, Loss: 0.005158, Acc: 76.94 391/391 \n",
      "\n",
      "Test Loss: 0.0085, Acc: 64.41%\n",
      "\n",
      " [=============================>]  Step: 178ms | Tot: 6s740ms | Train Epoch: 6, Loss: 0.004339, Acc: 80.58 391/391 \n",
      "\n",
      "Test Loss: 0.0078, Acc: 68.59%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s568ms | Train Epoch: 7, Loss: 0.003588, Acc: 83.85 391/391 \n",
      "\n",
      "Test Loss: 0.0087, Acc: 64.55%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s902ms | Train Epoch: 8, Loss: 0.002963, Acc: 86.49 391/391 \n",
      "\n",
      "Test Loss: 0.0095, Acc: 66.05%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s557ms | Train Epoch: 9, Loss: 0.002419, Acc: 88.99 391/391 \n",
      "\n",
      "Test Loss: 0.0096, Acc: 66.48%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s651ms | Train Epoch: 10, Loss: 0.001996, Acc: 91.00 391/391 \n",
      "\n",
      "Test Loss: 0.0089, Acc: 69.59%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s711ms | Train Epoch: 11, Loss: 0.001642, Acc: 92.56 391/391 \n",
      "\n",
      "Test Loss: 0.0098, Acc: 68.00%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s714ms | Train Epoch: 12, Loss: 0.001350, Acc: 93.92 391/391 \n",
      "\n",
      "Test Loss: 0.0116, Acc: 66.12%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s642ms | Train Epoch: 13, Loss: 0.001065, Acc: 95.22 391/391 \n",
      "\n",
      "Test Loss: 0.0110, Acc: 69.73%\n",
      "\n",
      " [=============================>]  Step: 168ms | Tot: 6s642ms | Train Epoch: 14, Loss: 0.000967, Acc: 95.70 391/391 \n",
      "\n",
      "Test Loss: 0.0115, Acc: 68.32%\n",
      "\n",
      " [=============================>]  Step: 170ms | Tot: 6s736ms | Train Epoch: 15, Loss: 0.000735, Acc: 96.73 391/391 \n",
      "\n",
      "Test Loss: 0.0120, Acc: 70.13%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s726ms | Train Epoch: 16, Loss: 0.000661, Acc: 96.98 391/391 \n",
      "\n",
      "Test Loss: 0.0117, Acc: 69.34%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s640ms | Train Epoch: 17, Loss: 0.000558, Acc: 97.51 391/391 \n",
      "\n",
      "Test Loss: 0.0152, Acc: 65.05%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s686ms | Train Epoch: 18, Loss: 0.000498, Acc: 97.78 391/391 \n",
      "\n",
      "Test Loss: 0.0134, Acc: 69.91%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s694ms | Train Epoch: 19, Loss: 0.000444, Acc: 98.04 391/391 \n",
      "\n",
      "Test Loss: 0.0120, Acc: 72.63%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s674ms | Train Epoch: 20, Loss: 0.000334, Acc: 98.55 391/391 \n",
      "\n",
      "Test Loss: 0.0143, Acc: 70.62%\n",
      "\n",
      " [=============================>]  Step: 167ms | Tot: 6s738ms | Train Epoch: 21, Loss: 0.000299, Acc: 98.70 391/391 \n",
      "\n",
      "Test Loss: 0.0128, Acc: 72.18%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s698ms | Train Epoch: 22, Loss: 0.000331, Acc: 98.54 391/391 \n",
      "\n",
      "Test Loss: 0.0130, Acc: 72.18%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s653ms | Train Epoch: 23, Loss: 0.000225, Acc: 98.94 391/391 \n",
      "\n",
      "Test Loss: 0.0138, Acc: 71.30%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s827ms | Train Epoch: 24, Loss: 0.000205, Acc: 99.10 391/391 \n",
      "\n",
      "Test Loss: 0.0133, Acc: 72.74%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s722ms | Train Epoch: 25, Loss: 0.000164, Acc: 99.29 391/391 \n",
      "\n",
      "Test Loss: 0.0148, Acc: 71.59%\n",
      "\n",
      " [=============================>]  Step: 168ms | Tot: 6s773ms | Train Epoch: 26, Loss: 0.000182, Acc: 99.24 391/391 \n",
      "\n",
      "Test Loss: 0.0134, Acc: 73.89%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s740ms | Train Epoch: 27, Loss: 0.000168, Acc: 99.30 391/391 \n",
      "\n",
      "Test Loss: 0.0158, Acc: 70.42%\n",
      "\n",
      " [=============================>]  Step: 170ms | Tot: 6s760ms | Train Epoch: 28, Loss: 0.000124, Acc: 99.51 391/391 \n",
      "\n",
      "Test Loss: 0.0143, Acc: 73.17%\n",
      "\n",
      " [=============================>]  Step: 174ms | Tot: 6s900ms | Train Epoch: 29, Loss: 0.000100, Acc: 99.58 391/391 \n",
      "\n",
      "Test Loss: 0.0142, Acc: 73.91%\n",
      "\n",
      " [=============================>]  Step: 167ms | Tot: 6s713ms | Train Epoch: 30, Loss: 0.000080, Acc: 99.68 391/391 \n",
      "\n",
      "Test Loss: 0.0147, Acc: 73.79%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s710ms | Train Epoch: 31, Loss: 0.000102, Acc: 99.58 391/391 \n",
      "\n",
      "Test Loss: 0.0143, Acc: 74.02%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s614ms | Train Epoch: 32, Loss: 0.000072, Acc: 99.69 391/391 \n",
      "\n",
      "Test Loss: 0.0143, Acc: 75.02%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s676ms | Train Epoch: 33, Loss: 0.000050, Acc: 99.80 391/391 \n",
      "\n",
      "Test Loss: 0.0149, Acc: 73.91%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s722ms | Train Epoch: 34, Loss: 0.000032, Acc: 99.88 391/391 \n",
      "\n",
      "Test Loss: 0.0148, Acc: 74.61%\n",
      "\n",
      " [=============================>]  Step: 167ms | Tot: 6s685ms | Train Epoch: 35, Loss: 0.000027, Acc: 99.89 391/391 41/391 \n",
      "\n",
      "Test Loss: 0.0165, Acc: 73.94%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s678ms | Train Epoch: 36, Loss: 0.000024, Acc: 99.91 391/391 \n",
      "\n",
      "Test Loss: 0.0285, Acc: 61.07%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s689ms | Train Epoch: 37, Loss: 0.000036, Acc: 99.84 391/391 \n",
      "\n",
      "Test Loss: 0.0158, Acc: 74.25%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s653ms | Train Epoch: 38, Loss: 0.000020, Acc: 99.93 391/391 \n",
      "\n",
      "Test Loss: 0.0157, Acc: 75.22%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s714ms | Train Epoch: 39, Loss: 0.000012, Acc: 99.95 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 74.08%\n",
      "\n",
      " [=============================>]  Step: 167ms | Tot: 6s696ms | Train Epoch: 40, Loss: 0.000013, Acc: 99.95 391/391 \n",
      "\n",
      "Test Loss: 0.0173, Acc: 73.14%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s760ms | Train Epoch: 41, Loss: 0.000016, Acc: 99.93 391/391 \n",
      "\n",
      "Test Loss: 0.0158, Acc: 75.46%\n",
      "\n",
      " [=============================>]  Step: 167ms | Tot: 6s643ms | Train Epoch: 42, Loss: 0.000012, Acc: 99.96 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 74.88%\n",
      "\n",
      " [=============================>]  Step: 169ms | Tot: 6s753ms | Train Epoch: 43, Loss: 0.000006, Acc: 99.98 391/391 \n",
      "\n",
      "Test Loss: 0.0161, Acc: 75.41%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s711ms | Train Epoch: 44, Loss: 0.000005, Acc: 99.98 391/391 \n",
      "\n",
      "Test Loss: 0.0165, Acc: 75.12%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s628ms | Train Epoch: 45, Loss: 0.000003, Acc: 99.99 391/391 \n",
      "\n",
      "Test Loss: 0.0164, Acc: 75.51%\n",
      "\n",
      " [=============================>]  Step: 168ms | Tot: 6s667ms | Train Epoch: 46, Loss: 0.000002, Acc: 99.99 391/391 \n",
      "\n",
      "Test Loss: 0.0166, Acc: 75.44%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s761ms | Train Epoch: 47, Loss: 0.000002, Acc: 99.99 391/391 \n",
      "\n",
      "Test Loss: 0.0163, Acc: 75.34%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s674ms | Train Epoch: 48, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0165, Acc: 75.37%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s688ms | Train Epoch: 49, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0163, Acc: 75.61%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s594ms | Train Epoch: 50, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.40%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s646ms | Train Epoch: 51, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.39%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s739ms | Train Epoch: 52, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.35%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s617ms | Train Epoch: 53, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.57%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s690ms | Train Epoch: 54, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.57%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>]  Step: 165ms | Tot: 6s688ms | Train Epoch: 55, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.48%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s607ms | Train Epoch: 56, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.72%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s642ms | Train Epoch: 57, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.68%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s700ms | Train Epoch: 58, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0166, Acc: 75.66%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s703ms | Train Epoch: 59, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.66%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s673ms | Train Epoch: 60, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.89%\n",
      "\n",
      " [=============================>]  Step: 171ms | Tot: 6s748ms | Train Epoch: 61, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.90%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s714ms | Train Epoch: 62, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.75%\n",
      "\n",
      " [=============================>]  Step: 170ms | Tot: 6s712ms | Train Epoch: 63, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.71%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s762ms | Train Epoch: 64, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.58%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s705ms | Train Epoch: 65, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.86%\n",
      "\n",
      " [=============================>]  Step: 167ms | Tot: 6s639ms | Train Epoch: 66, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.86%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s687ms | Train Epoch: 67, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.79%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s663ms | Train Epoch: 68, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.77%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s604ms | Train Epoch: 69, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.86%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s695ms | Train Epoch: 70, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0173, Acc: 75.82%\n",
      "\n",
      " [=============================>]  Step: 167ms | Tot: 6s629ms | Train Epoch: 71, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.76%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s691ms | Train Epoch: 72, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.69%\n",
      "\n",
      " [=============================>]  Step: 170ms | Tot: 6s683ms | Train Epoch: 73, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0173, Acc: 75.69%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s738ms | Train Epoch: 74, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.70%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s743ms | Train Epoch: 75, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.81%\n",
      "\n",
      " [=============================>]  Step: 167ms | Tot: 6s642ms | Train Epoch: 76, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0173, Acc: 75.72%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s700ms | Train Epoch: 77, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0174, Acc: 75.55%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s665ms | Train Epoch: 78, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.73%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s670ms | Train Epoch: 79, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0173, Acc: 75.84%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s745ms | Train Epoch: 80, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.65%\n",
      "\n",
      " [=============================>]  Step: 167ms | Tot: 6s652ms | Train Epoch: 81, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.71%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s686ms | Train Epoch: 82, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.63%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s637ms | Train Epoch: 83, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0173, Acc: 75.82%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s586ms | Train Epoch: 84, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0173, Acc: 75.64%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s680ms | Train Epoch: 85, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.69%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s641ms | Train Epoch: 86, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.68%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s723ms | Train Epoch: 87, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0173, Acc: 75.71%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s639ms | Train Epoch: 88, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0173, Acc: 75.81%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s604ms | Train Epoch: 89, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.76%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s619ms | Train Epoch: 90, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.75%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s713ms | Train Epoch: 91, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.86%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s695ms | Train Epoch: 92, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.82%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s792ms | Train Epoch: 93, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0174, Acc: 75.66%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s638ms | Train Epoch: 94, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.81%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s741ms | Train Epoch: 95, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.79%\n",
      "\n",
      " [=============================>]  Step: 168ms | Tot: 6s640ms | Train Epoch: 96, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0173, Acc: 75.65%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s697ms | Train Epoch: 97, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0175, Acc: 75.74%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s695ms | Train Epoch: 98, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.74%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s648ms | Train Epoch: 99, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.81%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s736ms | Train Epoch: 100, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0172, Acc: 75.68%\n",
      "\n",
      "Elapsed time: 0:13:08.896236\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12741/4039384257.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m   \u001b[0;31m# Replace the model with the Accelerate API.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m#pred = torch.argmax(model(image.to(device)), dim=1).cpu().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "run.display(height=1000)\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "best_acc = 0\n",
    "for epoch in range(wandb.config[\"epochs\"]):\n",
    "    train_acc, train_loss = train(epoch)\n",
    "    test_acc, test_loss = test()\n",
    "    if test_acc > best_acc:\n",
    "        wandb.run.summary[\"Best accuracy\"] = test_acc\n",
    "        best_acc = test_acc\n",
    "        accelerator.save(model, \"resnet18_best_acc.pth\")\n",
    "    wandb.log({\n",
    "        \"Train accuracy\": train_acc,\n",
    "        \"Test accuracy\": test_acc,\n",
    "        \"Train loss\": train_loss,\n",
    "        \"Test loss\": test_loss,\n",
    "        \"Learning rate\": optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "\n",
    "elapsed_time = datetime.datetime.now() - start_time\n",
    "print(\"Elapsed time: %s\" % elapsed_time)\n",
    "wandb.run.summary[\"Elapsed train time\"] = str(elapsed_time)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  pred = torch.argmax(model(image), dim=1).cpu().numpy()\n",
    "  # Replace the model with the Accelerate API.\n",
    "  #pred = torch.argmax(model(image.to(device)), dim=1).cpu().numpy()\n",
    "\n",
    "final_pred = []\n",
    "for i in range(8):\n",
    "    final_pred.append(label_human[pred[i]])\n",
    "    print(label_human[label[i]], \"vs. \",  final_pred[i])\n",
    "\n",
    "table_test.add_column(name=\"Final Pred Label\", data=final_pred)\n",
    "\n",
    "wandb.log({\"Test data\": table_test})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the best performing model\n",
    "\n",
    "In the following code, we load the best performing model. The model is saved in `./resnet18_best_acc.pth`. The average accuracy of the model is the same as the one in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.0168, Acc: 75.90%\n",
      "\n",
      "Best accuracy: 75.90\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"resnet18_best_acc.pth\")\n",
    "# Using Accelerator API\n",
    "model = accelerator.prepare(model)\n",
    "accuracy, _ = test()\n",
    "print(\"Best accuracy: %.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52772734322c44a04e342c358be70f2ff4d97da358cb3cd38ceb0f6066598be5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
