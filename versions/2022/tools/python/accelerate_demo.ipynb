{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Accelerate Demo\n",
    "\n",
    "**Note**: Before running this demo, please make sure that you have `wandb.ai` free account. \n",
    "\n",
    "Let us install `accelerate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/rowel/anaconda3/lib/python3.7/site-packages (0.5.1)\r\n",
      "Requirement already satisfied: pyyaml in /home/rowel/anaconda3/lib/python3.7/site-packages (from accelerate) (6.0)\r\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/rowel/anaconda3/lib/python3.7/site-packages (from accelerate) (1.10.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/rowel/anaconda3/lib/python3.7/site-packages (from accelerate) (1.21.2)\r\n",
      "Requirement already satisfied: typing-extensions in /home/rowel/anaconda3/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (3.10.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import** the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import wandb\n",
    "import datetime\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from ui import progress_bar\n",
    "\n",
    "# This is a demo of the PyTorch Accelerate API.\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`wandb`** initialization. See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrowel\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2022-03-12 20:54:48.217804: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-12 20:54:48.217847: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rowel/github/roatienza/Deep-Learning-Experiments/versions/2022/tools/python/wandb/run-20220312_205446-1bjoh6ee</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/upeee/accelerate-project/runs/1bjoh6ee\" target=\"_blank\">ruby-resonance-8</a></strong> to <a href=\"https://wandb.ai/upeee/accelerate-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "config = {\n",
    "  \"learning_rate\": 0.1,\n",
    "  \"epochs\": 100,\n",
    "  \"batch_size\": 128,\n",
    "  \"dataset\": \"cifar10\"\n",
    "}\n",
    "run = wandb.init(project=\"accelerate-project\", entity=\"upeee\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "Use a ResNet18 from `torchvision`. See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the code to be replaced with the Accelerate API.\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=False, progress=True)\n",
    "\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 10) \n",
    "\n",
    "# Replace the model with the Accelerate API.\n",
    "#model.to(device)\n",
    "\n",
    "# watch model gradients during training\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function, Optimizer, Scheduler and DataLoader\n",
    "\n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=wandb.config.learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=wandb.config.epochs)\n",
    "\n",
    "x_train = datasets.CIFAR10(root='./data', train=True, \n",
    "                           download=True, \n",
    "                           transform=transforms.ToTensor())\n",
    "x_test = datasets.CIFAR10(root='./data',\n",
    "                          train=False, \n",
    "                          download=True, \n",
    "                          transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(x_train,\n",
    "                          batch_size=wandb.config.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "test_loader = DataLoader(x_test, \n",
    "                         batch_size=wandb.config.batch_size, \n",
    "                         shuffle=False, \n",
    "                         num_workers=2)\n",
    "\n",
    "# Accelerate API\n",
    "model = accelerator.prepare(model)\n",
    "optimizer = accelerator.prepare(optimizer)\n",
    "scheduler = accelerator.prepare(scheduler)\n",
    "train_loader = accelerator.prepare(train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visulaizing sample data from test split\n",
    "\n",
    "\n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details.\n",
    "\n",
    "\n",
    "Note the last line that uses Accelerate API to wrap the model, optimizer, data loaders and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat vs.  bird\n",
      "ship vs.  bird\n",
      "ship vs.  bird\n",
      "airplane vs.  bird\n",
      "frog vs.  bird\n",
      "frog vs.  bird\n",
      "automobile vs.  bird\n",
      "frog vs.  bird\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_human = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "table_test = wandb.Table(columns=['Image', \"Ground Truth\", \"Initial Pred Label\",])\n",
    "\n",
    "image, label = iter(test_loader).next()\n",
    "test_loader = accelerator.prepare(test_loader)\n",
    "image = image.to(accelerator.device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  pred = torch.argmax(model(image), dim=1).cpu().numpy()\n",
    "\n",
    "for i in range(8):\n",
    "  table_test.add_data(wandb.Image(image[i]),\n",
    "                      label_human[label[i]], \n",
    "                      label_human[pred[i]])\n",
    "  print(label_human[label[i]], \"vs. \",  label_human[pred[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The train loop\n",
    "\n",
    "Using Accelerate, we do not need to transfer the model to the `device`.\n",
    "\n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  model.train()\n",
    "  train_loss = 0\n",
    "  correct = 0\n",
    "  train_samples = 0\n",
    "\n",
    "  # sample a batch. compute loss and backpropagate\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    # Replaced by the Accelerate API.\n",
    "    #target = target.to(device)\n",
    "    #output = model(data.to(device))\n",
    "    \n",
    "    output = model(data)\n",
    "    loss_value = loss(output, target)\n",
    "\n",
    "    # Replaced by the Accelerate API.\n",
    "    #loss_value.backward()\n",
    "    accelerator.backward(loss_value)\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step(epoch)\n",
    "    train_loss += loss_value.item()\n",
    "    train_samples += len(data)\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    if batch_idx % 10 == 0:\n",
    "      accuracy = 100. * correct / len(train_loader.dataset)\n",
    "      progress_bar(batch_idx,\n",
    "                   len(train_loader),\n",
    "                   'Train Epoch: {}, Loss: {:.6f}, Acc: {:.2f}%'.format(epoch+1, \n",
    "                   train_loss/train_samples, accuracy))\n",
    "  \n",
    "  train_loss /= len(train_loader.dataset)\n",
    "  accuracy = 100. * correct / len(train_loader.dataset)\n",
    "\n",
    "  return accuracy, train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The validation loop\n",
    "\n",
    "After every epoch, we will run the validation loop for the model. Again, no need to transfer the data to the `device`.\n",
    "\n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "\n",
    "      # Replaced by the Accelerate API.\n",
    "      #output = model(data.to(device))   \n",
    "      #target = target.to(device)\n",
    "\n",
    "      output = model(data)\n",
    "      test_loss += loss(output, target).item()\n",
    "      pred = output.argmax(dim=1, keepdim=True)\n",
    "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "  print('\\nTest Loss: {:.4f}, Acc: {:.2f}%\\n'.format(test_loss, accuracy))\n",
    "\n",
    "  return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `wandb` plots\n",
    "\n",
    "Finally, we will use `wandb` to visualize the training progress. \n",
    "See [`wandb_demo`](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/wandb_demo.ipynb) notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rowel/anaconda3/lib/python3.7/site-packages/IPython/core/display.py:724: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/upeee/accelerate-project/runs/1bjoh6ee?jupyter=true\" style=\"border:none;width:100%;height:1000px;\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [>.............................]  Step: 15s24ms | Tot: 0ms | Train Epoch: 1, Loss: 0.019370, Acc: 0.03%\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 1/391 \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rowel/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>]  Step: 163ms | Tot: 6s733ms | Train Epoch: 1, Loss: 0.013032, Acc: 44.19 391/391 \n",
      "\n",
      "Test Loss: 0.0101, Acc: 53.99%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s574ms | Train Epoch: 2, Loss: 0.008935, Acc: 59.64 391/391 \n",
      "\n",
      "Test Loss: 0.0130, Acc: 46.23%\n",
      "\n",
      " [=============================>]  Step: 167ms | Tot: 6s599ms | Train Epoch: 3, Loss: 0.007254, Acc: 67.17 391/391 \n",
      "\n",
      "Test Loss: 0.0120, Acc: 52.00%\n",
      "\n",
      " [=============================>]  Step: 167ms | Tot: 6s612ms | Train Epoch: 4, Loss: 0.006131, Acc: 72.55 391/391 \n",
      "\n",
      "Test Loss: 0.0113, Acc: 55.62%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s553ms | Train Epoch: 5, Loss: 0.005271, Acc: 76.14 391/391 \n",
      "\n",
      "Test Loss: 0.0121, Acc: 52.32%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s721ms | Train Epoch: 6, Loss: 0.004501, Acc: 79.66 391/391  \n",
      "\n",
      "Test Loss: 0.0114, Acc: 58.94%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s583ms | Train Epoch: 7, Loss: 0.003756, Acc: 83.12 391/391 \n",
      "\n",
      "Test Loss: 0.0088, Acc: 64.92%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s559ms | Train Epoch: 8, Loss: 0.003132, Acc: 85.91 391/391 \n",
      "\n",
      "Test Loss: 0.0089, Acc: 66.77%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s495ms | Train Epoch: 9, Loss: 0.002588, Acc: 88.24 391/391 \n",
      "\n",
      "Test Loss: 0.0089, Acc: 67.32%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s577ms | Train Epoch: 10, Loss: 0.002121, Acc: 90.50 391/391 \n",
      "\n",
      "Test Loss: 0.0098, Acc: 67.89%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s670ms | Train Epoch: 11, Loss: 0.001762, Acc: 91.93 391/391 \n",
      "\n",
      "Test Loss: 0.0113, Acc: 65.47%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s586ms | Train Epoch: 12, Loss: 0.001403, Acc: 93.71 391/391 \n",
      "\n",
      "Test Loss: 0.0144, Acc: 63.65%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s624ms | Train Epoch: 13, Loss: 0.001168, Acc: 94.67 391/391 \n",
      "\n",
      "Test Loss: 0.0134, Acc: 65.37%\n",
      "\n",
      " [=============================>]  Step: 160ms | Tot: 6s609ms | Train Epoch: 14, Loss: 0.001033, Acc: 95.34 391/391 \n",
      "\n",
      "Test Loss: 0.0101, Acc: 71.40%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s601ms | Train Epoch: 15, Loss: 0.000829, Acc: 96.28 391/391 \n",
      "\n",
      "Test Loss: 0.0119, Acc: 68.64%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s655ms | Train Epoch: 16, Loss: 0.000704, Acc: 96.86 391/391 \n",
      "\n",
      "Test Loss: 0.0111, Acc: 70.87%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s670ms | Train Epoch: 17, Loss: 0.000605, Acc: 97.33 391/391 \n",
      "\n",
      "Test Loss: 0.0130, Acc: 68.07%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s620ms | Train Epoch: 18, Loss: 0.000517, Acc: 97.74 391/391 \n",
      "\n",
      "Test Loss: 0.0150, Acc: 65.80%\n",
      "\n",
      " [=============================>]  Step: 160ms | Tot: 6s488ms | Train Epoch: 19, Loss: 0.000461, Acc: 97.94 391/391 \n",
      "\n",
      "Test Loss: 0.0120, Acc: 71.35%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s625ms | Train Epoch: 20, Loss: 0.000388, Acc: 98.25 391/391 \n",
      "\n",
      "Test Loss: 0.0147, Acc: 67.76%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s651ms | Train Epoch: 21, Loss: 0.000371, Acc: 98.31 391/391 \n",
      "\n",
      "Test Loss: 0.0137, Acc: 71.03%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s644ms | Train Epoch: 22, Loss: 0.000259, Acc: 98.86 391/391 \n",
      "\n",
      "Test Loss: 0.0135, Acc: 71.95%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s620ms | Train Epoch: 23, Loss: 0.000247, Acc: 98.92 391/391 \n",
      "\n",
      "Test Loss: 0.0180, Acc: 65.46%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s651ms | Train Epoch: 24, Loss: 0.000225, Acc: 99.01 391/391 \n",
      "\n",
      "Test Loss: 0.0141, Acc: 71.32%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s623ms | Train Epoch: 25, Loss: 0.000212, Acc: 99.10 391/391 \n",
      "\n",
      "Test Loss: 0.0134, Acc: 73.09%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s679ms | Train Epoch: 26, Loss: 0.000195, Acc: 99.16 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 68.81%\n",
      "\n",
      " [=============================>]  Step: 159ms | Tot: 6s460ms | Train Epoch: 27, Loss: 0.000223, Acc: 99.01 391/391 \n",
      "\n",
      "Test Loss: 0.0131, Acc: 72.19%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s553ms | Train Epoch: 28, Loss: 0.000120, Acc: 99.49 391/391 \n",
      "\n",
      "Test Loss: 0.0139, Acc: 72.02%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s681ms | Train Epoch: 29, Loss: 0.000119, Acc: 99.50 391/391 \n",
      "\n",
      "Test Loss: 0.0162, Acc: 70.20%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s554ms | Train Epoch: 30, Loss: 0.000091, Acc: 99.61 391/391 \n",
      "\n",
      "Test Loss: 0.0140, Acc: 73.52%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s655ms | Train Epoch: 31, Loss: 0.000061, Acc: 99.75 391/391 \n",
      "\n",
      "Test Loss: 0.0140, Acc: 73.72%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s562ms | Train Epoch: 32, Loss: 0.000057, Acc: 99.78 391/391 \n",
      "\n",
      "Test Loss: 0.0145, Acc: 73.71%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s583ms | Train Epoch: 33, Loss: 0.000046, Acc: 99.84 391/391 \n",
      "\n",
      "Test Loss: 0.0145, Acc: 74.36%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s607ms | Train Epoch: 34, Loss: 0.000028, Acc: 99.90 391/391 \n",
      "\n",
      "Test Loss: 0.0146, Acc: 74.34%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s611ms | Train Epoch: 35, Loss: 0.000016, Acc: 99.95 391/391 \n",
      "\n",
      "Test Loss: 0.0150, Acc: 74.16%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s636ms | Train Epoch: 36, Loss: 0.000008, Acc: 99.97 391/391 \n",
      "\n",
      "Test Loss: 0.0151, Acc: 74.42%\n",
      "\n",
      " [=============================>]  Step: 166ms | Tot: 6s668ms | Train Epoch: 37, Loss: 0.000007, Acc: 99.98 391/391 \n",
      "\n",
      "Test Loss: 0.0157, Acc: 74.43%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s611ms | Train Epoch: 38, Loss: 0.000005, Acc: 99.99 391/391 \n",
      "\n",
      "Test Loss: 0.0156, Acc: 74.58%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s627ms | Train Epoch: 39, Loss: 0.000003, Acc: 99.99 391/391 \n",
      "\n",
      "Test Loss: 0.0155, Acc: 75.13%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s598ms | Train Epoch: 40, Loss: 0.000002, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0156, Acc: 75.03%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s637ms | Train Epoch: 41, Loss: 0.000002, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0157, Acc: 75.07%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s553ms | Train Epoch: 42, Loss: 0.000002, Acc: 99.99 391/391 \n",
      "\n",
      "Test Loss: 0.0159, Acc: 74.86%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s565ms | Train Epoch: 43, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0158, Acc: 75.08%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s543ms | Train Epoch: 44, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0162, Acc: 75.00%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s588ms | Train Epoch: 45, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0161, Acc: 74.85%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s556ms | Train Epoch: 46, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0160, Acc: 74.95%\n",
      "\n",
      " [=============================>]  Step: 160ms | Tot: 6s581ms | Train Epoch: 47, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0162, Acc: 75.09%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s557ms | Train Epoch: 48, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0165, Acc: 74.94%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s551ms | Train Epoch: 49, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0163, Acc: 74.81%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s597ms | Train Epoch: 50, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0164, Acc: 74.84%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s637ms | Train Epoch: 51, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0163, Acc: 74.91%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s618ms | Train Epoch: 52, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0166, Acc: 75.00%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s577ms | Train Epoch: 53, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 74.79%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s649ms | Train Epoch: 54, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0165, Acc: 74.93%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=============================>]  Step: 165ms | Tot: 6s676ms | Train Epoch: 55, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 74.94%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s586ms | Train Epoch: 56, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 74.90%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s770ms | Train Epoch: 57, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0166, Acc: 74.95%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s607ms | Train Epoch: 58, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0166, Acc: 75.05%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s617ms | Train Epoch: 59, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 74.82%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s644ms | Train Epoch: 60, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.02%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s573ms | Train Epoch: 61, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.06%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s573ms | Train Epoch: 62, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0166, Acc: 74.95%\n",
      "\n",
      " [=============================>]  Step: 160ms | Tot: 6s615ms | Train Epoch: 63, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 74.92%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s620ms | Train Epoch: 64, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 74.89%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s602ms | Train Epoch: 65, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.03%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s556ms | Train Epoch: 66, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 74.88%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s620ms | Train Epoch: 67, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 74.96%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s644ms | Train Epoch: 68, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0165, Acc: 75.12%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s453ms | Train Epoch: 69, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 74.90%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s650ms | Train Epoch: 70, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 74.96%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s565ms | Train Epoch: 71, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.07%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s615ms | Train Epoch: 72, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.05%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s615ms | Train Epoch: 73, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 74.98%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s627ms | Train Epoch: 74, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.00%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s670ms | Train Epoch: 75, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 74.82%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s599ms | Train Epoch: 76, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.11%\n",
      "\n",
      " [=============================>]  Step: 165ms | Tot: 6s628ms | Train Epoch: 77, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 74.86%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s561ms | Train Epoch: 78, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.08%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s625ms | Train Epoch: 79, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.05%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s614ms | Train Epoch: 80, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.06%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s630ms | Train Epoch: 81, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.13%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s630ms | Train Epoch: 82, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.06%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s572ms | Train Epoch: 83, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.03%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s586ms | Train Epoch: 84, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 74.98%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s633ms | Train Epoch: 85, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 74.94%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s533ms | Train Epoch: 86, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.04%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s619ms | Train Epoch: 87, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.07%\n",
      "\n",
      " [=============================>]  Step: 160ms | Tot: 6s465ms | Train Epoch: 88, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.10%\n",
      "\n",
      " [=============================>]  Step: 167ms | Tot: 6s672ms | Train Epoch: 89, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0167, Acc: 75.19%\n",
      "\n",
      " [=============================>]  Step: 160ms | Tot: 6s657ms | Train Epoch: 90, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.00%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s546ms | Train Epoch: 91, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 74.91%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s483ms | Train Epoch: 92, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0171, Acc: 75.08%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s603ms | Train Epoch: 93, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.04%\n",
      "\n",
      " [=============================>]  Step: 164ms | Tot: 6s615ms | Train Epoch: 94, Loss: 0.000000, Acc: 100.00 391/391   Step: 200ms | Tot: 202ms | Train Epoch: 94, Loss: 0.000001, Acc: 2.82 11/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.13%\n",
      "\n",
      " [=============================>]  Step: 163ms | Tot: 6s557ms | Train Epoch: 95, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 74.97%\n",
      "\n",
      " [=============================>]  Step: 159ms | Tot: 6s481ms | Train Epoch: 96, Loss: 0.000001, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.07%\n",
      "\n",
      " [=============================>]  Step: 161ms | Tot: 6s591ms | Train Epoch: 97, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0168, Acc: 75.08%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s601ms | Train Epoch: 98, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0170, Acc: 75.00%\n",
      "\n",
      " [=============================>]  Step: 160ms | Tot: 6s551ms | Train Epoch: 99, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.12%\n",
      "\n",
      " [=============================>]  Step: 162ms | Tot: 6s627ms | Train Epoch: 100, Loss: 0.000000, Acc: 100.00 391/391 \n",
      "\n",
      "Test Loss: 0.0169, Acc: 75.16%\n",
      "\n",
      "Elapsed time: 0:13:03.392685\n",
      "cat vs.  cat\n",
      "ship vs.  automobile\n",
      "ship vs.  ship\n",
      "airplane vs.  airplane\n",
      "frog vs.  frog\n",
      "frog vs.  frog\n",
      "automobile vs.  automobile\n",
      "frog vs.  frog\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.125 MB of 0.125 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning rate</td><td>████████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Test accuracy</td><td>▂▁▃▅▅▅▇▅▇▅▆▇████████████████████████████</td></tr><tr><td>Test loss</td><td>▂▃▃▁▃▄▃▆▅█▇▅▅▅▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train accuracy</td><td>▁▄▅▆▇▇██████████████████████████████████</td></tr><tr><td>Train loss</td><td>█▅▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best accuracy</td><td>75.19</td></tr><tr><td>Elapsed train time</td><td>0:13:03.392685</td></tr><tr><td>Learning rate</td><td>2e-05</td></tr><tr><td>Test accuracy</td><td>75.16</td></tr><tr><td>Test loss</td><td>0.01693</td></tr><tr><td>Train accuracy</td><td>100.0</td></tr><tr><td>Train loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">ruby-resonance-8</strong>: <a href=\"https://wandb.ai/upeee/accelerate-project/runs/1bjoh6ee\" target=\"_blank\">https://wandb.ai/upeee/accelerate-project/runs/1bjoh6ee</a><br/>Synced 7 W&B file(s), 1 media file(s), 12 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220312_205446-1bjoh6ee/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.display(height=1000)\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "best_acc = 0\n",
    "for epoch in range(wandb.config[\"epochs\"]):\n",
    "    train_acc, train_loss = train(epoch)\n",
    "    test_acc, test_loss = test()\n",
    "    if test_acc > best_acc:\n",
    "        wandb.run.summary[\"Best accuracy\"] = test_acc\n",
    "        best_acc = test_acc\n",
    "        accelerator.save(model, \"resnet18_best_acc.pth\")\n",
    "    wandb.log({\n",
    "        \"Train accuracy\": train_acc,\n",
    "        \"Test accuracy\": test_acc,\n",
    "        \"Train loss\": train_loss,\n",
    "        \"Test loss\": test_loss,\n",
    "        \"Learning rate\": optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "\n",
    "elapsed_time = datetime.datetime.now() - start_time\n",
    "print(\"Elapsed time: %s\" % elapsed_time)\n",
    "wandb.run.summary[\"Elapsed train time\"] = str(elapsed_time)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  pred = torch.argmax(model(image), dim=1).cpu().numpy()\n",
    "\n",
    "final_pred = []\n",
    "for i in range(8):\n",
    "    final_pred.append(label_human[pred[i]])\n",
    "    print(label_human[label[i]], \"vs. \",  final_pred[i])\n",
    "\n",
    "table_test.add_column(name=\"Final Pred Label\", data=final_pred)\n",
    "\n",
    "wandb.log({\"Test data\": table_test})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the best performing model\n",
    "\n",
    "In the following code, we load the best performing model. The model is saved in `./resnet18_best_acc.pth`. The average accuracy of the model is the same as the one in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.0167, Acc: 75.19%\n",
      "\n",
      "Best accuracy: 75.19\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"resnet18_best_acc.pth\")\n",
    "# Using Accelerator API\n",
    "model = accelerator.prepare(model)\n",
    "accuracy, _ = test()\n",
    "print(\"Best accuracy: %.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52772734322c44a04e342c358be70f2ff4d97da358cb3cd38ceb0f6066598be5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
