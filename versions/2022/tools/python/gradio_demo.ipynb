{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2693f215",
   "metadata": {},
   "source": [
    "### Gradio and HuggingFace\n",
    "\n",
    "In this demo, we show how to build ready to deploy or use deep learning models. \n",
    "\n",
    "Huggingface hosts thousands of pre-trained models. They also built high-level APIs so we can easily use and deploy pre-trained models. \n",
    "\n",
    "`gradio` provides APIs so we can easily build web applications that use our pre-trained models from Huggingface. `gradio` also provides APIs so we can easily incorporate input and output web UIs.\n",
    "\n",
    "After building the `gradio`, we can have permanent hosting using Huggingface Spaces. \n",
    "\n",
    "Let us first install Huggingface `transformers` and `gradio`.\n",
    "\n",
    "**Note:** For some examples, it is best to launch the app in another tab to enable access to the required inputs such as microphone or webcam. Running the app may also lock the `python` kernel and the notebook becomes unresponsive. In that case, please restart the kernel and clear the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f575d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471cee1e",
   "metadata": {},
   "source": [
    "#### Hello world in gradio\n",
    "\n",
    "As a tradition, let us build the simplest `gradio` app. It accepts a `text` input and calls the `greet()` function to process this input.\n",
    "\n",
    "To finally see our application, we call `launch()` after constructing our `gradio` `Interface`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcfbdee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7863/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe2b35c9820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<fastapi.applications.FastAPI at 0x7fe2b0b0ea00>,\n",
       " 'http://127.0.0.1:7863/',\n",
       " None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name + \"!!\"\n",
    "\n",
    "gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d57f084",
   "metadata": {},
   "source": [
    "#### Object Recognition using ResNet18\n",
    "\n",
    "In our discussion about PyTorch, we used a pre-trained ResNet18 model from `torchvision`. We use `jupyter` notebook to show the results. The `jupyter` notebook is not an application that we can deploy and other people use with ease. The same with Google's colab. \n",
    "\n",
    "In this example, we use `gradio` to build a simple app that an end user can easily interact with. We reuse the code from our previous [example](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/versions/2022/tools/python/pytorch_demo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a78f2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7864/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe299f7b910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<fastapi.applications.FastAPI at 0x7fe2b0b0ea00>,\n",
       " 'http://127.0.0.1:7864/',\n",
       " None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import requests\n",
    "from einops import rearrange\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "resnet.eval()\n",
    "\n",
    "# Download human-readable labels for ImageNet.\n",
    "response = requests.get(\"https://git.io/JJkYN\")\n",
    "labels = response.text.split(\"\\n\")\n",
    "\n",
    "def classify(img):\n",
    "    # By default, gradio image is numpy\n",
    "    img = torch.from_numpy(img)\n",
    "    # Numpy image is channel last. PyTorch is channel 1st.\n",
    "    img = rearrange(img, 'h w c -> c h w')\n",
    "    \n",
    "    # The transforms before prediction\n",
    "    img = torchvision.transforms.Resize(256)(img)\n",
    "    img = torchvision.transforms.CenterCrop(224)(img).float()/255.\n",
    "    img = normalize(img)\n",
    "    \n",
    "    # We insert batch size of 1\n",
    "    img = rearrange(img, 'c h w  -> 1 c h w')\n",
    "    \n",
    "    # The actual prediction\n",
    "    with torch.no_grad():\n",
    "        pred = resnet(img)\n",
    "    \n",
    "    # Convert the prediction to probabilities\n",
    "    pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "    # Remove the batch dim. torch.squeeze() can also be used.\n",
    "    pred = rearrange(pred, \"1 j->j\")\n",
    "    \n",
    "    # torch to numpy space\n",
    "    pred = pred.cpu().numpy()\n",
    "    \n",
    "    return {labels[i]: float(pred[i]) for i in range(1000)}\n",
    "    \n",
    "# Input to ResNet18\n",
    "image = gr.inputs.Image(shape=(224, 224))\n",
    "# We print top5 classes\n",
    "label = gr.outputs.Label(num_top_classes=5)\n",
    "examples = ['data/wonder_cat.jpg', 'data/aki_dog.jpg',]\n",
    "\n",
    "gr.Interface(fn=classify, \n",
    "             inputs=image, \n",
    "             outputs=label, \n",
    "             title=\"1k Object Recognition\",\n",
    "             examples = ['wonder_cat.jpg', 'aki_dog.jpg',],\n",
    "             description=\"Demonstrates how to use a pre-trained model from torchvision for image classification.\",\n",
    "             ).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f068a96",
   "metadata": {},
   "source": [
    "#### Using HuggingFace and Gradio\n",
    "\n",
    "Loading a pre-trained model from torchvision, pre-processing the input, and post processing the output are all messy. Sometimes, we just want to load and use a machine learning model. Huggingface provides a shortcut for all these steps through the use of `pipeline`. In `pipeline`, we supply the task name and the pre-trained model that is stored in Huggingface Model Hub.\n",
    "\n",
    "In this example, we use a much better model compared to ResNet18. It is called [BEIT](https://arxiv.org/abs/2106.08254) and can classify objects up to about 22k categories. We construct the `gradio` app by calling `from_pipeline()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b00c8a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rowel/anaconda3/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7865/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe29f8b7220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<fastapi.applications.FastAPI at 0x7fe2b0b0ea00>,\n",
       " 'http://127.0.0.1:7865/',\n",
       " None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"image-classification\", \n",
    "                 # model that can do 22k-category classification\n",
    "                 model=\"microsoft/beit-base-patch16-224-pt22k-ft22k\")\n",
    "gr.Interface.from_pipeline(pipe, \n",
    "                           title=\"22k Image Classification\",\n",
    "                           description=\"Object Recognition using Microsoft BEIT (22k classes)\",\n",
    "                           examples = ['wonder_cat.jpg', 'aki_dog.jpg',],\n",
    "                           ).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741196f",
   "metadata": {},
   "source": [
    "#### Automatic Speech Recognition (ASR)\n",
    "\n",
    "Let us shift to audio or speech domain. In this example, we demonstrate an Automatic Speech Recognition (ASR) system. We will use our microphone to record audio which is then converted to text using ASR. In this example, best to open the application in another browser tab by setting `inbrowser=True`.\n",
    "\n",
    "Before running the `gradio` app, this ASR requires `sentencepice` module. Let us install it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d953972",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61145737",
   "metadata": {},
   "source": [
    "In this ASR, we use Huggingface pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df57123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7866/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe2a01cf850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<fastapi.applications.FastAPI at 0x7fe2b0b0ea00>,\n",
       " 'http://127.0.0.1:7866/',\n",
       " None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rowel/anaconda3/lib/python3.8/site-packages/transformers/models/speech_to_text/modeling_speech_to_text.py:566: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  input_lengths = (input_lengths - 1) // 2 + 1\n",
      "/Users/rowel/anaconda3/lib/python3.8/site-packages/transformers/models/speech_to_text/modeling_speech_to_text.py:566: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  input_lengths = (input_lengths - 1) // 2 + 1\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "model = pipeline(task=\"automatic-speech-recognition\", \n",
    "                 model=\"facebook/s2t-medium-librispeech-asr\")\n",
    "gr.Interface.from_pipeline(model,\n",
    "                           title=\"Automatic Speech Recognition (ASR)\",\n",
    "                           description=\"Using pipeline with Facebook S2T for ASR.\",\n",
    "                           examples=['data/ljspeech.wav',],\n",
    "                           ).launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46c9a0",
   "metadata": {},
   "source": [
    "#### Text to Speech (TTS)\n",
    "\n",
    "Let us do the reverse of ASR or Text to Speech (TTS). In this example, we supply text and this text is converted to speech using the voice of Linda Johnson. We use a pre-trained model of [FastSpeech2](https://arxiv.org/abs/2006.04558) that is provided by Facebook in Model Hub.\n",
    "\n",
    "In this example, we use [`load()`](https://gradio.app/docs/#load) method to load the pre-trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a990c161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching model from: https://huggingface.co/facebook/fastspeech2-en-ljspeech\n",
      "Running on local URL:  http://127.0.0.1:7867/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7867/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe271a47550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<fastapi.applications.FastAPI at 0x7fe2b0b0ea00>,\n",
       " 'http://127.0.0.1:7867/',\n",
       " None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "gr.Interface.load(\"huggingface/facebook/fastspeech2-en-ljspeech\", \n",
    "                  description=\"TTS using FastSpeech2\",\n",
    "                  title=\"Text to Speech (TTS)\",\n",
    "                  examples=[[\"The quick brown fox jumps over the lazy dog.\"]]\n",
    "                  ).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff8f1a",
   "metadata": {},
   "source": [
    "#### Text Generation using GPT2\n",
    "\n",
    "In this example, we use a large language model (LLM) by OpenAI to generate text. It is called [GPT2](https://github.com/openai/gpt-2). Text generation is one of the tasks where a language model can help us. Basically, we provide the initial text made of few words. Then, the model will continue it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52daa8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching model from: https://huggingface.co/gpt2\n",
      "Running on local URL:  http://127.0.0.1:7868/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7868/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe29f25d310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<fastapi.applications.FastAPI at 0x7fe2b0b0ea00>,\n",
       " 'http://127.0.0.1:7868/',\n",
       " None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "gr.Interface.load(\"huggingface/gpt2\",\n",
    "                  title=\"Text Generation\",\n",
    "                  description=\"Using GPT2.\",\n",
    "                  ).launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e42324c",
   "metadata": {},
   "source": [
    "#### Using `gr.Series()` to automatically chain the I/O of multiple models\n",
    "\n",
    "In this example, we use the previous text generator as input to our TTS. We can for example use to generate a podcast on a certain topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1b1d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching model from: https://huggingface.co/gpt2\n",
      "Fetching model from: https://huggingface.co/facebook/fastspeech2-en-ljspeech\n",
      "Running on local URL:  http://127.0.0.1:7869/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7869/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe271dbe460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "textgen = gr.Interface.load(\"huggingface/gpt2\",\n",
    "                            title=\"Text Generation\",\n",
    "                            description=\"Using GPT2.\",\n",
    "                            )\n",
    "\n",
    "tts = gr.Interface.load(\"huggingface/facebook/fastspeech2-en-ljspeech\", \n",
    "                        description=\"TTS using FastSpeech2\",\n",
    "                        title=\"Text to Speech (TTS)\",\n",
    "                        examples=[[\"The quick brown fox jumps over the lazy dog.\"]]\n",
    "                        )\n",
    "\n",
    "gr.Series(textgen, tts).launch(show_error=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f415559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
